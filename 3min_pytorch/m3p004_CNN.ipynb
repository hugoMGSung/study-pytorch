{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN으로 패션아이템 구분\n",
    "- Convolutional Neural Network (CNN) 을 이용하여 패션아이템 구분 성능 고도화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms, datasets\n",
    "\n",
    "from tqdm.autonotebook import tqdm # 프로그래스바 시각화 모듈"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_CUDA = torch.cuda.is_available()\n",
    "DEVICE = torch.device('cuda' if USE_CUDA else 'cpu')\n",
    "\n",
    "EPOCHS     = 40\n",
    "BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 데이터셋 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.FashionMNIST('./data/',\n",
    "                   train=True,\n",
    "                   download=True,\n",
    "                   transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ])),\n",
    "    batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.FashionMNIST('./data/',\n",
    "                   train=False, \n",
    "                   transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ])),\n",
    "    batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 뉴럴넷으로 Fashion MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Network, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "        self.conv2_drop = nn.Dropout2d()\n",
    "        self.fc1 = nn.Linear(320, 50)\n",
    "        self.fc2 = nn.Linear(50, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
    "        x = x.view(-1, 320)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 하이퍼파라미터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model     = Network().to(DEVICE)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(DEVICE), target.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.cross_entropy(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch_idx % 200 == 0:\n",
    "            print(f'Train Epoch: {epoch} [{batch_idx * len(data)}/{len(train_loader.dataset)} ' + \\\n",
    "                  f'({100. * batch_idx / len(train_loader):.0f}%)]\\tLoss: {loss.item():.6f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(DEVICE), target.to(DEVICE)\n",
    "            output = model(data)\n",
    "\n",
    "            # 배치 오차를 합산\n",
    "            test_loss += F.cross_entropy(output, target,\n",
    "                                         reduction='sum').item()\n",
    "\n",
    "            # 가장 높은 값을 가진 인덱스가 바로 예측값\n",
    "            pred = output.max(1, keepdim=True)[1]\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    test_accuracy = 100. * correct / len(test_loader.dataset)\n",
    "    return test_loss, test_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 코드 실행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f5e315c3e274893828f4919f8882ac1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.313828\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 1.689898\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 1.069033\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 1.230990\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.718220\n",
      "[1] Test Loss: 0.7136, Accuracy: 72.44%\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.867169\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.902843\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.850940\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.723771\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.793350\n",
      "[2] Test Loss: 0.5926, Accuracy: 75.60%\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.702417\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.688312\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.850773\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.846347\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.550859\n",
      "[3] Test Loss: 0.5408, Accuracy: 79.26%\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.738840\n",
      "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 0.716539\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.762709\n",
      "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 0.562285\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.534882\n",
      "[4] Test Loss: 0.4978, Accuracy: 80.76%\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.559599\n",
      "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 0.533036\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 0.337640\n",
      "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 0.585298\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.465814\n",
      "[5] Test Loss: 0.4779, Accuracy: 82.05%\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.547614\n",
      "Train Epoch: 6 [12800/60000 (21%)]\tLoss: 0.863605\n",
      "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 0.796098\n",
      "Train Epoch: 6 [38400/60000 (64%)]\tLoss: 0.733872\n",
      "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 0.491522\n",
      "[6] Test Loss: 0.4622, Accuracy: 82.90%\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.614549\n",
      "Train Epoch: 7 [12800/60000 (21%)]\tLoss: 0.527852\n",
      "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 0.604897\n",
      "Train Epoch: 7 [38400/60000 (64%)]\tLoss: 0.634802\n",
      "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 0.459523\n",
      "[7] Test Loss: 0.4399, Accuracy: 83.46%\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.348131\n",
      "Train Epoch: 8 [12800/60000 (21%)]\tLoss: 0.592807\n",
      "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 0.534229\n",
      "Train Epoch: 8 [38400/60000 (64%)]\tLoss: 0.640001\n",
      "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 0.555524\n",
      "[8] Test Loss: 0.4210, Accuracy: 84.48%\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.625669\n",
      "Train Epoch: 9 [12800/60000 (21%)]\tLoss: 0.465780\n",
      "Train Epoch: 9 [25600/60000 (43%)]\tLoss: 0.483253\n",
      "Train Epoch: 9 [38400/60000 (64%)]\tLoss: 0.784472\n",
      "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 0.311871\n",
      "[9] Test Loss: 0.4160, Accuracy: 84.51%\n",
      "Train Epoch: 10 [0/60000 (0%)]\tLoss: 0.623212\n",
      "Train Epoch: 10 [12800/60000 (21%)]\tLoss: 0.657957\n",
      "Train Epoch: 10 [25600/60000 (43%)]\tLoss: 0.546193\n",
      "Train Epoch: 10 [38400/60000 (64%)]\tLoss: 0.421457\n",
      "Train Epoch: 10 [51200/60000 (85%)]\tLoss: 0.428091\n",
      "[10] Test Loss: 0.4058, Accuracy: 84.95%\n",
      "Train Epoch: 11 [0/60000 (0%)]\tLoss: 0.389968\n",
      "Train Epoch: 11 [12800/60000 (21%)]\tLoss: 0.463209\n",
      "Train Epoch: 11 [25600/60000 (43%)]\tLoss: 0.407160\n",
      "Train Epoch: 11 [38400/60000 (64%)]\tLoss: 0.683306\n",
      "Train Epoch: 11 [51200/60000 (85%)]\tLoss: 0.258977\n",
      "[11] Test Loss: 0.3970, Accuracy: 85.01%\n",
      "Train Epoch: 12 [0/60000 (0%)]\tLoss: 0.421751\n",
      "Train Epoch: 12 [12800/60000 (21%)]\tLoss: 0.442924\n",
      "Train Epoch: 12 [25600/60000 (43%)]\tLoss: 0.478057\n",
      "Train Epoch: 12 [38400/60000 (64%)]\tLoss: 0.300913\n",
      "Train Epoch: 12 [51200/60000 (85%)]\tLoss: 0.440523\n",
      "[12] Test Loss: 0.3934, Accuracy: 85.39%\n",
      "Train Epoch: 13 [0/60000 (0%)]\tLoss: 0.558444\n",
      "Train Epoch: 13 [12800/60000 (21%)]\tLoss: 0.339206\n",
      "Train Epoch: 13 [25600/60000 (43%)]\tLoss: 0.436464\n",
      "Train Epoch: 13 [38400/60000 (64%)]\tLoss: 0.440169\n",
      "Train Epoch: 13 [51200/60000 (85%)]\tLoss: 0.639563\n",
      "[13] Test Loss: 0.3792, Accuracy: 85.98%\n",
      "Train Epoch: 14 [0/60000 (0%)]\tLoss: 0.393768\n",
      "Train Epoch: 14 [12800/60000 (21%)]\tLoss: 0.460403\n",
      "Train Epoch: 14 [25600/60000 (43%)]\tLoss: 0.598081\n",
      "Train Epoch: 14 [38400/60000 (64%)]\tLoss: 0.387111\n",
      "Train Epoch: 14 [51200/60000 (85%)]\tLoss: 0.397792\n",
      "[14] Test Loss: 0.3825, Accuracy: 85.70%\n",
      "Train Epoch: 15 [0/60000 (0%)]\tLoss: 0.586258\n",
      "Train Epoch: 15 [12800/60000 (21%)]\tLoss: 0.605564\n",
      "Train Epoch: 15 [25600/60000 (43%)]\tLoss: 0.603436\n",
      "Train Epoch: 15 [38400/60000 (64%)]\tLoss: 0.587390\n",
      "Train Epoch: 15 [51200/60000 (85%)]\tLoss: 0.710935\n",
      "[15] Test Loss: 0.3727, Accuracy: 85.69%\n",
      "Train Epoch: 16 [0/60000 (0%)]\tLoss: 0.466972\n",
      "Train Epoch: 16 [12800/60000 (21%)]\tLoss: 0.432707\n",
      "Train Epoch: 16 [25600/60000 (43%)]\tLoss: 0.315710\n",
      "Train Epoch: 16 [38400/60000 (64%)]\tLoss: 0.462119\n",
      "Train Epoch: 16 [51200/60000 (85%)]\tLoss: 0.392339\n",
      "[16] Test Loss: 0.3680, Accuracy: 86.32%\n",
      "Train Epoch: 17 [0/60000 (0%)]\tLoss: 0.537976\n",
      "Train Epoch: 17 [12800/60000 (21%)]\tLoss: 0.494391\n",
      "Train Epoch: 17 [25600/60000 (43%)]\tLoss: 0.507481\n",
      "Train Epoch: 17 [38400/60000 (64%)]\tLoss: 0.531496\n",
      "Train Epoch: 17 [51200/60000 (85%)]\tLoss: 0.600680\n",
      "[17] Test Loss: 0.3701, Accuracy: 86.01%\n",
      "Train Epoch: 18 [0/60000 (0%)]\tLoss: 0.552710\n",
      "Train Epoch: 18 [12800/60000 (21%)]\tLoss: 0.410765\n",
      "Train Epoch: 18 [25600/60000 (43%)]\tLoss: 0.525301\n",
      "Train Epoch: 18 [38400/60000 (64%)]\tLoss: 0.472012\n",
      "Train Epoch: 18 [51200/60000 (85%)]\tLoss: 0.503620\n",
      "[18] Test Loss: 0.3632, Accuracy: 86.45%\n",
      "Train Epoch: 19 [0/60000 (0%)]\tLoss: 0.421714\n",
      "Train Epoch: 19 [12800/60000 (21%)]\tLoss: 0.544888\n",
      "Train Epoch: 19 [25600/60000 (43%)]\tLoss: 0.459058\n",
      "Train Epoch: 19 [38400/60000 (64%)]\tLoss: 0.430602\n",
      "Train Epoch: 19 [51200/60000 (85%)]\tLoss: 0.353653\n",
      "[19] Test Loss: 0.3613, Accuracy: 86.27%\n",
      "Train Epoch: 20 [0/60000 (0%)]\tLoss: 0.336418\n",
      "Train Epoch: 20 [12800/60000 (21%)]\tLoss: 0.547783\n",
      "Train Epoch: 20 [25600/60000 (43%)]\tLoss: 0.289454\n",
      "Train Epoch: 20 [38400/60000 (64%)]\tLoss: 0.367881\n",
      "Train Epoch: 20 [51200/60000 (85%)]\tLoss: 0.357679\n",
      "[20] Test Loss: 0.3524, Accuracy: 86.67%\n",
      "Train Epoch: 21 [0/60000 (0%)]\tLoss: 0.455582\n",
      "Train Epoch: 21 [12800/60000 (21%)]\tLoss: 0.544306\n",
      "Train Epoch: 21 [25600/60000 (43%)]\tLoss: 0.460622\n",
      "Train Epoch: 21 [38400/60000 (64%)]\tLoss: 0.365418\n",
      "Train Epoch: 21 [51200/60000 (85%)]\tLoss: 0.406905\n",
      "[21] Test Loss: 0.3534, Accuracy: 86.60%\n",
      "Train Epoch: 22 [0/60000 (0%)]\tLoss: 0.529008\n",
      "Train Epoch: 22 [12800/60000 (21%)]\tLoss: 0.335560\n",
      "Train Epoch: 22 [25600/60000 (43%)]\tLoss: 0.445388\n",
      "Train Epoch: 22 [38400/60000 (64%)]\tLoss: 0.464190\n",
      "Train Epoch: 22 [51200/60000 (85%)]\tLoss: 0.413540\n",
      "[22] Test Loss: 0.3541, Accuracy: 86.91%\n",
      "Train Epoch: 23 [0/60000 (0%)]\tLoss: 0.617907\n",
      "Train Epoch: 23 [12800/60000 (21%)]\tLoss: 0.397074\n",
      "Train Epoch: 23 [25600/60000 (43%)]\tLoss: 0.406133\n",
      "Train Epoch: 23 [38400/60000 (64%)]\tLoss: 0.782082\n",
      "Train Epoch: 23 [51200/60000 (85%)]\tLoss: 0.527848\n",
      "[23] Test Loss: 0.3452, Accuracy: 87.36%\n",
      "Train Epoch: 24 [0/60000 (0%)]\tLoss: 0.327256\n",
      "Train Epoch: 24 [12800/60000 (21%)]\tLoss: 0.331068\n",
      "Train Epoch: 24 [25600/60000 (43%)]\tLoss: 0.282557\n",
      "Train Epoch: 24 [38400/60000 (64%)]\tLoss: 0.366877\n",
      "Train Epoch: 24 [51200/60000 (85%)]\tLoss: 0.462703\n",
      "[24] Test Loss: 0.3485, Accuracy: 86.86%\n",
      "Train Epoch: 25 [0/60000 (0%)]\tLoss: 0.373927\n",
      "Train Epoch: 25 [12800/60000 (21%)]\tLoss: 0.399755\n",
      "Train Epoch: 25 [25600/60000 (43%)]\tLoss: 0.274308\n",
      "Train Epoch: 25 [38400/60000 (64%)]\tLoss: 0.348986\n",
      "Train Epoch: 25 [51200/60000 (85%)]\tLoss: 0.492762\n",
      "[25] Test Loss: 0.3436, Accuracy: 87.13%\n",
      "Train Epoch: 26 [0/60000 (0%)]\tLoss: 0.432128\n",
      "Train Epoch: 26 [12800/60000 (21%)]\tLoss: 0.387261\n",
      "Train Epoch: 26 [25600/60000 (43%)]\tLoss: 0.691464\n",
      "Train Epoch: 26 [38400/60000 (64%)]\tLoss: 0.289293\n",
      "Train Epoch: 26 [51200/60000 (85%)]\tLoss: 0.444515\n",
      "[26] Test Loss: 0.3409, Accuracy: 86.96%\n",
      "Train Epoch: 27 [0/60000 (0%)]\tLoss: 0.384235\n",
      "Train Epoch: 27 [12800/60000 (21%)]\tLoss: 0.450106\n",
      "Train Epoch: 27 [25600/60000 (43%)]\tLoss: 0.304206\n",
      "Train Epoch: 27 [38400/60000 (64%)]\tLoss: 0.490649\n",
      "Train Epoch: 27 [51200/60000 (85%)]\tLoss: 0.450038\n",
      "[27] Test Loss: 0.3413, Accuracy: 87.27%\n",
      "Train Epoch: 28 [0/60000 (0%)]\tLoss: 0.600510\n",
      "Train Epoch: 28 [12800/60000 (21%)]\tLoss: 0.562317\n",
      "Train Epoch: 28 [25600/60000 (43%)]\tLoss: 0.457596\n",
      "Train Epoch: 28 [38400/60000 (64%)]\tLoss: 0.463762\n",
      "Train Epoch: 28 [51200/60000 (85%)]\tLoss: 0.453282\n",
      "[28] Test Loss: 0.3397, Accuracy: 87.02%\n",
      "Train Epoch: 29 [0/60000 (0%)]\tLoss: 0.409600\n",
      "Train Epoch: 29 [12800/60000 (21%)]\tLoss: 0.319799\n",
      "Train Epoch: 29 [25600/60000 (43%)]\tLoss: 0.691708\n",
      "Train Epoch: 29 [38400/60000 (64%)]\tLoss: 0.451870\n",
      "Train Epoch: 29 [51200/60000 (85%)]\tLoss: 0.365849\n",
      "[29] Test Loss: 0.3354, Accuracy: 87.70%\n",
      "Train Epoch: 30 [0/60000 (0%)]\tLoss: 0.384597\n",
      "Train Epoch: 30 [12800/60000 (21%)]\tLoss: 0.479592\n",
      "Train Epoch: 30 [25600/60000 (43%)]\tLoss: 0.373907\n",
      "Train Epoch: 30 [38400/60000 (64%)]\tLoss: 0.360932\n",
      "Train Epoch: 30 [51200/60000 (85%)]\tLoss: 0.454057\n",
      "[30] Test Loss: 0.3344, Accuracy: 87.52%\n",
      "Train Epoch: 31 [0/60000 (0%)]\tLoss: 0.411035\n",
      "Train Epoch: 31 [12800/60000 (21%)]\tLoss: 0.432761\n",
      "Train Epoch: 31 [25600/60000 (43%)]\tLoss: 0.353092\n",
      "Train Epoch: 31 [38400/60000 (64%)]\tLoss: 0.372877\n",
      "Train Epoch: 31 [51200/60000 (85%)]\tLoss: 0.673736\n",
      "[31] Test Loss: 0.3293, Accuracy: 87.78%\n",
      "Train Epoch: 32 [0/60000 (0%)]\tLoss: 0.272374\n",
      "Train Epoch: 32 [12800/60000 (21%)]\tLoss: 0.325032\n",
      "Train Epoch: 32 [25600/60000 (43%)]\tLoss: 0.373786\n",
      "Train Epoch: 32 [38400/60000 (64%)]\tLoss: 0.489579\n",
      "Train Epoch: 32 [51200/60000 (85%)]\tLoss: 0.508719\n",
      "[32] Test Loss: 0.3294, Accuracy: 87.32%\n",
      "Train Epoch: 33 [0/60000 (0%)]\tLoss: 0.391425\n",
      "Train Epoch: 33 [12800/60000 (21%)]\tLoss: 0.349969\n",
      "Train Epoch: 33 [25600/60000 (43%)]\tLoss: 0.410764\n",
      "Train Epoch: 33 [38400/60000 (64%)]\tLoss: 0.414922\n",
      "Train Epoch: 33 [51200/60000 (85%)]\tLoss: 0.453914\n",
      "[33] Test Loss: 0.3363, Accuracy: 87.07%\n",
      "Train Epoch: 34 [0/60000 (0%)]\tLoss: 0.369784\n",
      "Train Epoch: 34 [12800/60000 (21%)]\tLoss: 0.464026\n",
      "Train Epoch: 34 [25600/60000 (43%)]\tLoss: 0.271686\n",
      "Train Epoch: 34 [38400/60000 (64%)]\tLoss: 0.425559\n",
      "Train Epoch: 34 [51200/60000 (85%)]\tLoss: 0.288227\n",
      "[34] Test Loss: 0.3291, Accuracy: 87.82%\n",
      "Train Epoch: 35 [0/60000 (0%)]\tLoss: 0.307592\n",
      "Train Epoch: 35 [12800/60000 (21%)]\tLoss: 0.425361\n",
      "Train Epoch: 35 [25600/60000 (43%)]\tLoss: 0.431783\n",
      "Train Epoch: 35 [38400/60000 (64%)]\tLoss: 0.327951\n",
      "Train Epoch: 35 [51200/60000 (85%)]\tLoss: 0.411846\n",
      "[35] Test Loss: 0.3292, Accuracy: 87.50%\n",
      "Train Epoch: 36 [0/60000 (0%)]\tLoss: 0.401837\n",
      "Train Epoch: 36 [12800/60000 (21%)]\tLoss: 0.264964\n",
      "Train Epoch: 36 [25600/60000 (43%)]\tLoss: 0.502296\n",
      "Train Epoch: 36 [38400/60000 (64%)]\tLoss: 0.377755\n",
      "Train Epoch: 36 [51200/60000 (85%)]\tLoss: 0.405826\n",
      "[36] Test Loss: 0.3261, Accuracy: 87.59%\n",
      "Train Epoch: 37 [0/60000 (0%)]\tLoss: 0.375139\n",
      "Train Epoch: 37 [12800/60000 (21%)]\tLoss: 0.373140\n",
      "Train Epoch: 37 [25600/60000 (43%)]\tLoss: 0.302839\n",
      "Train Epoch: 37 [38400/60000 (64%)]\tLoss: 0.622335\n",
      "Train Epoch: 37 [51200/60000 (85%)]\tLoss: 0.486205\n",
      "[37] Test Loss: 0.3271, Accuracy: 87.55%\n",
      "Train Epoch: 38 [0/60000 (0%)]\tLoss: 0.414128\n",
      "Train Epoch: 38 [12800/60000 (21%)]\tLoss: 0.559489\n",
      "Train Epoch: 38 [25600/60000 (43%)]\tLoss: 0.412784\n",
      "Train Epoch: 38 [38400/60000 (64%)]\tLoss: 0.486016\n",
      "Train Epoch: 38 [51200/60000 (85%)]\tLoss: 0.369734\n",
      "[38] Test Loss: 0.3228, Accuracy: 87.88%\n",
      "Train Epoch: 39 [0/60000 (0%)]\tLoss: 0.398659\n",
      "Train Epoch: 39 [12800/60000 (21%)]\tLoss: 0.467159\n",
      "Train Epoch: 39 [25600/60000 (43%)]\tLoss: 0.297848\n",
      "Train Epoch: 39 [38400/60000 (64%)]\tLoss: 0.451596\n",
      "Train Epoch: 39 [51200/60000 (85%)]\tLoss: 0.661468\n",
      "[39] Test Loss: 0.3227, Accuracy: 87.77%\n",
      "Train Epoch: 40 [0/60000 (0%)]\tLoss: 0.411367\n",
      "Train Epoch: 40 [12800/60000 (21%)]\tLoss: 0.459237\n",
      "Train Epoch: 40 [25600/60000 (43%)]\tLoss: 0.400431\n",
      "Train Epoch: 40 [38400/60000 (64%)]\tLoss: 0.320302\n",
      "Train Epoch: 40 [51200/60000 (85%)]\tLoss: 0.487830\n",
      "[40] Test Loss: 0.3251, Accuracy: 87.60%\n"
     ]
    }
   ],
   "source": [
    "for epoch in tqdm( range(1, EPOCHS + 1) ):\n",
    "    train(model, train_loader, optimizer, epoch)\n",
    "    test_loss, test_accuracy = evaluate(model, test_loader)\n",
    "    \n",
    "    print(f'[{epoch}] Test Loss: {test_loss:.4f}, Accuracy: {test_accuracy:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 신경망 깊게 쌓아 컬러 데이터셋에 적용 - ResNet\n",
    "- 잔차 학습(Residual Learning) \n",
    "    - 일반적인 깊은 신경망에서는 층이 깊어질수록 기울기 소실(Vanishing Gradient) 문제가 발생해 학습이 어려워짐\n",
    "    - ResNet은 출력값을 학습하는 대신, 입력과 출력 간의 차이(잔차)를 학습하도록 설계\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_CUDA = torch.cuda.is_available()\n",
    "DEVICE = torch.device(\"cuda\" if USE_CUDA else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 하이퍼파라미터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS     = 300\n",
    "BATCH_SIZE = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 데이터셋 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 170498071/170498071 [00:11<00:00, 14736479.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/cifar-10-python.tar.gz to ./data/\n"
     ]
    }
   ],
   "source": [
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.CIFAR10('./data/',\n",
    "                   train=True,\n",
    "                   download=True,\n",
    "                   transform=transforms.Compose([\n",
    "                       transforms.RandomCrop(32, padding=4),\n",
    "                       transforms.RandomHorizontalFlip(),\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.5, 0.5, 0.5),\n",
    "                                            (0.5, 0.5, 0.5))])),\n",
    "    batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.CIFAR10('./data/',\n",
    "                   train=False, \n",
    "                   transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.5, 0.5, 0.5),\n",
    "                                            (0.5, 0.5, 0.5))])),\n",
    "    batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ResNet 모델 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicBlock(nn.Module):\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3,\n",
    "                               stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n",
    "                               stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, planes,\n",
    "                          kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(planes)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_planes = 16\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3,\n",
    "                               stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        self.layer1 = self._make_layer(16, 2, stride=1)\n",
    "        self.layer2 = self._make_layer(32, 2, stride=2)\n",
    "        self.layer3 = self._make_layer(64, 2, stride=2)\n",
    "        self.linear = nn.Linear(64, num_classes)\n",
    "\n",
    "    def _make_layer(self, planes, num_blocks, stride):\n",
    "        strides = [stride] + [1]*(num_blocks-1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(BasicBlock(self.in_planes, planes, stride))\n",
    "            self.in_planes = planes\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = F.avg_pool2d(out, 8)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.linear(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ResNet().to(DEVICE)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1,\n",
    "                      momentum=0.9, weight_decay=0.0005)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=50, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential(\n",
       "        (0): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential(\n",
       "        (0): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "  )\n",
       "  (linear): Linear(in_features=64, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(DEVICE), target.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.cross_entropy(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(DEVICE), target.to(DEVICE)\n",
    "            output = model(data)\n",
    "\n",
    "            # 배치 오차를 합산\n",
    "            test_loss += F.cross_entropy(output, target,\n",
    "                                         reduction='sum').item()\n",
    "\n",
    "            # 가장 높은 값을 가진 인덱스가 바로 예측값\n",
    "            pred = output.max(1, keepdim=True)[1]\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    test_accuracy = 100. * correct / len(test_loader.dataset)\n",
    "    return test_loss, test_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 코드 실행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "421c01abd9074ef5a17fe2c7a5e57814",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/300 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\perso\\.pyenv\\pyenv-win\\versions\\3.11.9\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] Test Loss: 1.4262, Accuracy: 46.81%\n",
      "[2] Test Loss: 1.1230, Accuracy: 60.13%\n",
      "[3] Test Loss: 1.3840, Accuracy: 56.87%\n",
      "[4] Test Loss: 0.8706, Accuracy: 70.68%\n",
      "[5] Test Loss: 0.8685, Accuracy: 70.88%\n",
      "[6] Test Loss: 0.9230, Accuracy: 69.26%\n",
      "[7] Test Loss: 0.8097, Accuracy: 73.30%\n",
      "[8] Test Loss: 0.9447, Accuracy: 69.96%\n",
      "[9] Test Loss: 1.2459, Accuracy: 62.06%\n",
      "[10] Test Loss: 0.6554, Accuracy: 78.17%\n",
      "[11] Test Loss: 1.5906, Accuracy: 58.05%\n",
      "[12] Test Loss: 0.7937, Accuracy: 74.25%\n",
      "[13] Test Loss: 0.7358, Accuracy: 75.42%\n",
      "[14] Test Loss: 1.0244, Accuracy: 67.35%\n",
      "[15] Test Loss: 0.8184, Accuracy: 74.23%\n",
      "[16] Test Loss: 0.7487, Accuracy: 75.80%\n",
      "[17] Test Loss: 0.8143, Accuracy: 74.45%\n",
      "[18] Test Loss: 1.0754, Accuracy: 68.10%\n",
      "[19] Test Loss: 0.6112, Accuracy: 79.45%\n",
      "[20] Test Loss: 0.6646, Accuracy: 78.45%\n",
      "[21] Test Loss: 0.9261, Accuracy: 70.19%\n",
      "[22] Test Loss: 0.8370, Accuracy: 71.09%\n",
      "[23] Test Loss: 0.6629, Accuracy: 78.06%\n",
      "[24] Test Loss: 0.8407, Accuracy: 73.45%\n",
      "[25] Test Loss: 0.7140, Accuracy: 76.45%\n",
      "[26] Test Loss: 0.9386, Accuracy: 71.87%\n",
      "[27] Test Loss: 0.7421, Accuracy: 75.23%\n",
      "[28] Test Loss: 0.5811, Accuracy: 80.41%\n",
      "[29] Test Loss: 0.7608, Accuracy: 75.35%\n",
      "[30] Test Loss: 0.6669, Accuracy: 78.50%\n",
      "[31] Test Loss: 0.6128, Accuracy: 79.73%\n",
      "[32] Test Loss: 0.6720, Accuracy: 78.99%\n",
      "[33] Test Loss: 0.7072, Accuracy: 77.64%\n",
      "[34] Test Loss: 0.7370, Accuracy: 77.09%\n",
      "[35] Test Loss: 0.7300, Accuracy: 76.14%\n",
      "[36] Test Loss: 0.7209, Accuracy: 76.12%\n",
      "[37] Test Loss: 0.6094, Accuracy: 79.70%\n",
      "[38] Test Loss: 0.7438, Accuracy: 77.05%\n",
      "[39] Test Loss: 1.1993, Accuracy: 64.35%\n",
      "[40] Test Loss: 0.5406, Accuracy: 81.74%\n",
      "[41] Test Loss: 0.6019, Accuracy: 79.85%\n",
      "[42] Test Loss: 0.7843, Accuracy: 74.32%\n",
      "[43] Test Loss: 0.7272, Accuracy: 76.12%\n",
      "[44] Test Loss: 0.7095, Accuracy: 77.02%\n",
      "[45] Test Loss: 0.6474, Accuracy: 78.63%\n",
      "[46] Test Loss: 0.7186, Accuracy: 76.56%\n",
      "[47] Test Loss: 0.6779, Accuracy: 77.67%\n",
      "[48] Test Loss: 0.8597, Accuracy: 73.49%\n",
      "[49] Test Loss: 0.5495, Accuracy: 81.81%\n",
      "[50] Test Loss: 0.3408, Accuracy: 88.64%\n",
      "[51] Test Loss: 0.3399, Accuracy: 88.42%\n",
      "[52] Test Loss: 0.3289, Accuracy: 89.12%\n",
      "[53] Test Loss: 0.3240, Accuracy: 89.09%\n",
      "[54] Test Loss: 0.3220, Accuracy: 89.22%\n",
      "[55] Test Loss: 0.3178, Accuracy: 89.41%\n",
      "[56] Test Loss: 0.3199, Accuracy: 89.36%\n",
      "[57] Test Loss: 0.3466, Accuracy: 88.97%\n",
      "[58] Test Loss: 0.3346, Accuracy: 88.78%\n",
      "[59] Test Loss: 0.3182, Accuracy: 89.55%\n",
      "[60] Test Loss: 0.3235, Accuracy: 89.24%\n",
      "[61] Test Loss: 0.3329, Accuracy: 89.35%\n",
      "[62] Test Loss: 0.3282, Accuracy: 89.21%\n",
      "[63] Test Loss: 0.3319, Accuracy: 89.13%\n",
      "[64] Test Loss: 0.3374, Accuracy: 89.08%\n",
      "[65] Test Loss: 0.3602, Accuracy: 88.83%\n",
      "[66] Test Loss: 0.3477, Accuracy: 89.21%\n",
      "[67] Test Loss: 0.3279, Accuracy: 89.57%\n",
      "[68] Test Loss: 0.3291, Accuracy: 89.48%\n",
      "[69] Test Loss: 0.3491, Accuracy: 88.94%\n",
      "[70] Test Loss: 0.3280, Accuracy: 89.21%\n",
      "[71] Test Loss: 0.3574, Accuracy: 88.65%\n",
      "[72] Test Loss: 0.3363, Accuracy: 89.31%\n",
      "[73] Test Loss: 0.3506, Accuracy: 88.61%\n",
      "[74] Test Loss: 0.3435, Accuracy: 88.98%\n",
      "[75] Test Loss: 0.3472, Accuracy: 89.28%\n",
      "[76] Test Loss: 0.3471, Accuracy: 88.92%\n",
      "[77] Test Loss: 0.3518, Accuracy: 88.60%\n",
      "[78] Test Loss: 0.3903, Accuracy: 87.89%\n",
      "[79] Test Loss: 0.3844, Accuracy: 87.91%\n",
      "[80] Test Loss: 0.4110, Accuracy: 87.82%\n",
      "[81] Test Loss: 0.3443, Accuracy: 89.27%\n",
      "[82] Test Loss: 0.3797, Accuracy: 88.14%\n",
      "[83] Test Loss: 0.3591, Accuracy: 88.62%\n",
      "[84] Test Loss: 0.3493, Accuracy: 88.61%\n",
      "[85] Test Loss: 0.4220, Accuracy: 86.91%\n",
      "[86] Test Loss: 0.3594, Accuracy: 88.59%\n",
      "[87] Test Loss: 0.4358, Accuracy: 86.82%\n",
      "[88] Test Loss: 0.4101, Accuracy: 87.52%\n",
      "[89] Test Loss: 0.3724, Accuracy: 88.17%\n",
      "[90] Test Loss: 0.4304, Accuracy: 86.92%\n",
      "[91] Test Loss: 0.3788, Accuracy: 88.13%\n",
      "[92] Test Loss: 0.3870, Accuracy: 87.35%\n",
      "[93] Test Loss: 0.4099, Accuracy: 87.69%\n",
      "[94] Test Loss: 0.3782, Accuracy: 88.19%\n",
      "[95] Test Loss: 0.4041, Accuracy: 87.36%\n",
      "[96] Test Loss: 0.4092, Accuracy: 87.32%\n",
      "[97] Test Loss: 0.3694, Accuracy: 88.65%\n",
      "[98] Test Loss: 0.3840, Accuracy: 88.12%\n",
      "[99] Test Loss: 0.4200, Accuracy: 87.37%\n",
      "[100] Test Loss: 0.2883, Accuracy: 91.00%\n",
      "[101] Test Loss: 0.2842, Accuracy: 91.08%\n",
      "[102] Test Loss: 0.2822, Accuracy: 91.32%\n",
      "[103] Test Loss: 0.2835, Accuracy: 91.35%\n",
      "[104] Test Loss: 0.2831, Accuracy: 91.33%\n",
      "[105] Test Loss: 0.2857, Accuracy: 91.43%\n",
      "[106] Test Loss: 0.2818, Accuracy: 91.49%\n",
      "[107] Test Loss: 0.2864, Accuracy: 91.37%\n",
      "[108] Test Loss: 0.2876, Accuracy: 91.32%\n",
      "[109] Test Loss: 0.2883, Accuracy: 91.24%\n",
      "[110] Test Loss: 0.2854, Accuracy: 91.24%\n",
      "[111] Test Loss: 0.2836, Accuracy: 91.42%\n",
      "[112] Test Loss: 0.2896, Accuracy: 91.36%\n",
      "[113] Test Loss: 0.2859, Accuracy: 91.37%\n",
      "[114] Test Loss: 0.2901, Accuracy: 91.30%\n",
      "[115] Test Loss: 0.2904, Accuracy: 91.34%\n",
      "[116] Test Loss: 0.2893, Accuracy: 91.30%\n",
      "[117] Test Loss: 0.2888, Accuracy: 91.34%\n",
      "[118] Test Loss: 0.2884, Accuracy: 91.38%\n",
      "[119] Test Loss: 0.2885, Accuracy: 91.45%\n",
      "[120] Test Loss: 0.2907, Accuracy: 91.47%\n",
      "[121] Test Loss: 0.2938, Accuracy: 91.22%\n",
      "[122] Test Loss: 0.2943, Accuracy: 91.35%\n",
      "[123] Test Loss: 0.2900, Accuracy: 91.33%\n",
      "[124] Test Loss: 0.2968, Accuracy: 91.14%\n",
      "[125] Test Loss: 0.3016, Accuracy: 91.19%\n",
      "[126] Test Loss: 0.2943, Accuracy: 91.47%\n",
      "[127] Test Loss: 0.2934, Accuracy: 91.33%\n",
      "[128] Test Loss: 0.2918, Accuracy: 91.32%\n",
      "[129] Test Loss: 0.2971, Accuracy: 91.42%\n",
      "[130] Test Loss: 0.2982, Accuracy: 91.16%\n",
      "[131] Test Loss: 0.3017, Accuracy: 91.31%\n",
      "[132] Test Loss: 0.2975, Accuracy: 91.26%\n",
      "[133] Test Loss: 0.2999, Accuracy: 91.30%\n",
      "[134] Test Loss: 0.2986, Accuracy: 91.35%\n",
      "[135] Test Loss: 0.2972, Accuracy: 91.21%\n",
      "[136] Test Loss: 0.2997, Accuracy: 91.23%\n",
      "[137] Test Loss: 0.3022, Accuracy: 91.00%\n",
      "[138] Test Loss: 0.3056, Accuracy: 91.27%\n",
      "[139] Test Loss: 0.3008, Accuracy: 91.38%\n",
      "[140] Test Loss: 0.3022, Accuracy: 91.29%\n",
      "[141] Test Loss: 0.3057, Accuracy: 91.31%\n",
      "[142] Test Loss: 0.3049, Accuracy: 91.14%\n",
      "[143] Test Loss: 0.3069, Accuracy: 90.98%\n",
      "[144] Test Loss: 0.3051, Accuracy: 91.11%\n",
      "[145] Test Loss: 0.3089, Accuracy: 91.26%\n",
      "[146] Test Loss: 0.3054, Accuracy: 91.21%\n",
      "[147] Test Loss: 0.3075, Accuracy: 91.00%\n",
      "[148] Test Loss: 0.3081, Accuracy: 91.14%\n",
      "[149] Test Loss: 0.3087, Accuracy: 90.95%\n",
      "[150] Test Loss: 0.3033, Accuracy: 90.95%\n",
      "[151] Test Loss: 0.3000, Accuracy: 91.12%\n",
      "[152] Test Loss: 0.3014, Accuracy: 91.36%\n",
      "[153] Test Loss: 0.3005, Accuracy: 91.13%\n",
      "[154] Test Loss: 0.3045, Accuracy: 91.23%\n",
      "[155] Test Loss: 0.3017, Accuracy: 91.24%\n",
      "[156] Test Loss: 0.3036, Accuracy: 91.16%\n",
      "[157] Test Loss: 0.3024, Accuracy: 91.12%\n",
      "[158] Test Loss: 0.3010, Accuracy: 91.19%\n",
      "[159] Test Loss: 0.3047, Accuracy: 91.24%\n",
      "[160] Test Loss: 0.3029, Accuracy: 91.18%\n",
      "[161] Test Loss: 0.3016, Accuracy: 91.30%\n",
      "[162] Test Loss: 0.3047, Accuracy: 91.23%\n",
      "[163] Test Loss: 0.3016, Accuracy: 91.30%\n",
      "[164] Test Loss: 0.3027, Accuracy: 91.18%\n",
      "[165] Test Loss: 0.3025, Accuracy: 91.04%\n",
      "[166] Test Loss: 0.3036, Accuracy: 91.18%\n",
      "[167] Test Loss: 0.3030, Accuracy: 91.25%\n",
      "[168] Test Loss: 0.3032, Accuracy: 91.24%\n",
      "[169] Test Loss: 0.3032, Accuracy: 91.18%\n",
      "[170] Test Loss: 0.3038, Accuracy: 91.19%\n",
      "[171] Test Loss: 0.3051, Accuracy: 91.17%\n",
      "[172] Test Loss: 0.3046, Accuracy: 91.21%\n",
      "[173] Test Loss: 0.3032, Accuracy: 91.14%\n",
      "[174] Test Loss: 0.3045, Accuracy: 91.17%\n",
      "[175] Test Loss: 0.3058, Accuracy: 91.32%\n",
      "[176] Test Loss: 0.3056, Accuracy: 91.19%\n",
      "[177] Test Loss: 0.3050, Accuracy: 91.14%\n",
      "[178] Test Loss: 0.3034, Accuracy: 91.15%\n",
      "[179] Test Loss: 0.3071, Accuracy: 91.14%\n",
      "[180] Test Loss: 0.3073, Accuracy: 91.18%\n",
      "[181] Test Loss: 0.3063, Accuracy: 91.15%\n",
      "[182] Test Loss: 0.3035, Accuracy: 91.13%\n",
      "[183] Test Loss: 0.3064, Accuracy: 91.12%\n",
      "[184] Test Loss: 0.3058, Accuracy: 91.08%\n",
      "[185] Test Loss: 0.3063, Accuracy: 91.19%\n",
      "[186] Test Loss: 0.3083, Accuracy: 91.12%\n",
      "[187] Test Loss: 0.3068, Accuracy: 91.26%\n",
      "[188] Test Loss: 0.3039, Accuracy: 91.16%\n",
      "[189] Test Loss: 0.3056, Accuracy: 91.00%\n",
      "[190] Test Loss: 0.3056, Accuracy: 91.12%\n",
      "[191] Test Loss: 0.3077, Accuracy: 91.02%\n",
      "[192] Test Loss: 0.3087, Accuracy: 91.10%\n",
      "[193] Test Loss: 0.3068, Accuracy: 91.14%\n",
      "[194] Test Loss: 0.3046, Accuracy: 91.11%\n",
      "[195] Test Loss: 0.3069, Accuracy: 91.21%\n",
      "[196] Test Loss: 0.3062, Accuracy: 91.20%\n",
      "[197] Test Loss: 0.3047, Accuracy: 91.13%\n",
      "[198] Test Loss: 0.3073, Accuracy: 91.18%\n",
      "[199] Test Loss: 0.3058, Accuracy: 91.14%\n",
      "[200] Test Loss: 0.3077, Accuracy: 91.13%\n",
      "[201] Test Loss: 0.3047, Accuracy: 91.20%\n",
      "[202] Test Loss: 0.3075, Accuracy: 91.16%\n",
      "[203] Test Loss: 0.3060, Accuracy: 91.07%\n",
      "[204] Test Loss: 0.3037, Accuracy: 91.22%\n",
      "[205] Test Loss: 0.3073, Accuracy: 91.20%\n",
      "[206] Test Loss: 0.3055, Accuracy: 91.16%\n",
      "[207] Test Loss: 0.3087, Accuracy: 91.11%\n",
      "[208] Test Loss: 0.3073, Accuracy: 91.21%\n",
      "[209] Test Loss: 0.3069, Accuracy: 91.12%\n",
      "[210] Test Loss: 0.3067, Accuracy: 91.16%\n",
      "[211] Test Loss: 0.3071, Accuracy: 91.29%\n",
      "[212] Test Loss: 0.3086, Accuracy: 91.11%\n",
      "[213] Test Loss: 0.3065, Accuracy: 91.24%\n",
      "[214] Test Loss: 0.3055, Accuracy: 91.33%\n",
      "[215] Test Loss: 0.3063, Accuracy: 91.17%\n",
      "[216] Test Loss: 0.3063, Accuracy: 91.26%\n",
      "[217] Test Loss: 0.3046, Accuracy: 91.22%\n",
      "[218] Test Loss: 0.3060, Accuracy: 91.20%\n",
      "[219] Test Loss: 0.3048, Accuracy: 91.24%\n",
      "[220] Test Loss: 0.3068, Accuracy: 91.22%\n",
      "[221] Test Loss: 0.3055, Accuracy: 91.18%\n",
      "[222] Test Loss: 0.3057, Accuracy: 91.19%\n",
      "[223] Test Loss: 0.3052, Accuracy: 91.26%\n",
      "[224] Test Loss: 0.3076, Accuracy: 91.20%\n",
      "[225] Test Loss: 0.3051, Accuracy: 91.19%\n",
      "[226] Test Loss: 0.3057, Accuracy: 91.25%\n",
      "[227] Test Loss: 0.3064, Accuracy: 91.19%\n",
      "[228] Test Loss: 0.3068, Accuracy: 91.12%\n",
      "[229] Test Loss: 0.3044, Accuracy: 91.21%\n",
      "[230] Test Loss: 0.3045, Accuracy: 91.21%\n",
      "[231] Test Loss: 0.3037, Accuracy: 91.33%\n",
      "[232] Test Loss: 0.3058, Accuracy: 91.26%\n",
      "[233] Test Loss: 0.3056, Accuracy: 91.19%\n",
      "[234] Test Loss: 0.3059, Accuracy: 91.22%\n",
      "[235] Test Loss: 0.3050, Accuracy: 91.10%\n",
      "[236] Test Loss: 0.3074, Accuracy: 91.27%\n",
      "[237] Test Loss: 0.3072, Accuracy: 91.29%\n",
      "[238] Test Loss: 0.3077, Accuracy: 91.25%\n",
      "[239] Test Loss: 0.3075, Accuracy: 91.17%\n",
      "[240] Test Loss: 0.3055, Accuracy: 91.16%\n",
      "[241] Test Loss: 0.3054, Accuracy: 91.22%\n",
      "[242] Test Loss: 0.3062, Accuracy: 91.22%\n",
      "[243] Test Loss: 0.3045, Accuracy: 91.18%\n",
      "[244] Test Loss: 0.3056, Accuracy: 91.23%\n",
      "[245] Test Loss: 0.3106, Accuracy: 91.05%\n",
      "[246] Test Loss: 0.3084, Accuracy: 91.26%\n",
      "[247] Test Loss: 0.3099, Accuracy: 91.17%\n",
      "[248] Test Loss: 0.3074, Accuracy: 91.25%\n",
      "[249] Test Loss: 0.3061, Accuracy: 91.22%\n",
      "[250] Test Loss: 0.3077, Accuracy: 91.20%\n",
      "[251] Test Loss: 0.3063, Accuracy: 91.26%\n",
      "[252] Test Loss: 0.3071, Accuracy: 91.19%\n",
      "[253] Test Loss: 0.3047, Accuracy: 91.24%\n",
      "[254] Test Loss: 0.3064, Accuracy: 91.28%\n",
      "[255] Test Loss: 0.3065, Accuracy: 91.29%\n",
      "[256] Test Loss: 0.3071, Accuracy: 91.22%\n",
      "[257] Test Loss: 0.3072, Accuracy: 91.36%\n",
      "[258] Test Loss: 0.3076, Accuracy: 91.15%\n",
      "[259] Test Loss: 0.3069, Accuracy: 91.13%\n",
      "[260] Test Loss: 0.3066, Accuracy: 91.18%\n",
      "[261] Test Loss: 0.3079, Accuracy: 91.15%\n",
      "[262] Test Loss: 0.3068, Accuracy: 91.22%\n",
      "[263] Test Loss: 0.3082, Accuracy: 91.18%\n",
      "[264] Test Loss: 0.3068, Accuracy: 91.13%\n",
      "[265] Test Loss: 0.3081, Accuracy: 91.15%\n",
      "[266] Test Loss: 0.3081, Accuracy: 91.17%\n",
      "[267] Test Loss: 0.3072, Accuracy: 91.22%\n",
      "[268] Test Loss: 0.3074, Accuracy: 91.14%\n",
      "[269] Test Loss: 0.3077, Accuracy: 91.15%\n",
      "[270] Test Loss: 0.3078, Accuracy: 91.16%\n",
      "[271] Test Loss: 0.3071, Accuracy: 91.25%\n",
      "[272] Test Loss: 0.3049, Accuracy: 91.27%\n",
      "[273] Test Loss: 0.3060, Accuracy: 91.15%\n",
      "[274] Test Loss: 0.3069, Accuracy: 91.23%\n",
      "[275] Test Loss: 0.3047, Accuracy: 91.26%\n",
      "[276] Test Loss: 0.3060, Accuracy: 91.19%\n",
      "[277] Test Loss: 0.3064, Accuracy: 91.19%\n",
      "[278] Test Loss: 0.3056, Accuracy: 91.23%\n",
      "[279] Test Loss: 0.3076, Accuracy: 91.29%\n",
      "[280] Test Loss: 0.3064, Accuracy: 91.21%\n",
      "[281] Test Loss: 0.3072, Accuracy: 91.26%\n",
      "[282] Test Loss: 0.3051, Accuracy: 91.26%\n",
      "[283] Test Loss: 0.3061, Accuracy: 91.22%\n",
      "[284] Test Loss: 0.3069, Accuracy: 91.19%\n",
      "[285] Test Loss: 0.3064, Accuracy: 91.18%\n",
      "[286] Test Loss: 0.3081, Accuracy: 91.29%\n",
      "[287] Test Loss: 0.3062, Accuracy: 91.07%\n",
      "[288] Test Loss: 0.3091, Accuracy: 91.16%\n",
      "[289] Test Loss: 0.3055, Accuracy: 91.20%\n",
      "[290] Test Loss: 0.3047, Accuracy: 91.19%\n",
      "[291] Test Loss: 0.3039, Accuracy: 91.17%\n",
      "[292] Test Loss: 0.3060, Accuracy: 91.31%\n",
      "[293] Test Loss: 0.3068, Accuracy: 91.27%\n",
      "[294] Test Loss: 0.3071, Accuracy: 91.21%\n",
      "[295] Test Loss: 0.3084, Accuracy: 91.18%\n",
      "[296] Test Loss: 0.3077, Accuracy: 91.20%\n",
      "[297] Test Loss: 0.3047, Accuracy: 91.14%\n",
      "[298] Test Loss: 0.3063, Accuracy: 91.21%\n",
      "[299] Test Loss: 0.3067, Accuracy: 91.28%\n",
      "[300] Test Loss: 0.3080, Accuracy: 91.24%\n"
     ]
    }
   ],
   "source": [
    "for epoch in tqdm( range(1, EPOCHS + 1) ):\n",
    "    scheduler.step()\n",
    "    train(model, train_loader, optimizer, epoch)\n",
    "    test_loss, test_accuracy = evaluate(model, test_loader)\n",
    "    \n",
    "    print(f'[{epoch}] Test Loss: {test_loss:.4f}, Accuracy: {test_accuracy:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 엄청난 시간 소요. 예상은 1시간 30분 정도였으나, Idle time 등 문제로 13시간 44분 소요\n",
    "- 정확도는 91.20 % 까지 올라감"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
