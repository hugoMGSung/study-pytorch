{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch 학습 레벨 2 - 002\n",
    "---\n",
    "\n",
    "### 순서\n",
    "- Tensor\n",
    "- _~~Augograd~~_\n",
    "- DataSet, DataLoader\n",
    "- Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autograd\n",
    "- Automatic gradient calculating API \n",
    "- forward(Forward Propagation: 순전파)와 backward(Backward Propagation: 역전파)가 가능하게 해줌\n",
    "- [참조](https://tutorials.pytorch.kr/beginner/basics/autogradqs_tutorial.html)\n",
    "\n",
    "```python\n",
    "import torch\n",
    "\n",
    "x = torch.ones(5)  # input tensor\n",
    "y = torch.zeros(3)  # expected output\n",
    "w = torch.randn(5, 3, requires_grad=True)\n",
    "b = torch.randn(3, requires_grad=True)\n",
    "z = torch.matmul(x, w)+b\n",
    "loss = torch.nn.functional.binary_cross_entropy_with_logits(z, y)\n",
    "```\n",
    "\n",
    "<img src=\"https://tutorials.pytorch.kr/_images/comp-graph.png\" width=\"700\" style=\"background:white;\">\n",
    "\n",
    "\n",
    "- 이 신경망에서, w(가중치)와 b(절편편)는 최적화를 해야 하는 매개변수\n",
    "    - requires_grad=True 를 사용해서 추적가능\n",
    "- 변화도(Gradient) 계산, 변화도 추적\n",
    "- 다음번 노드 계산에서 사용할 때, 좀더 나은 값을 변경(조정)되어 대입되어야 함 -> 역전파\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional\n",
    "\n",
    "x = torch.ones(5)   # 입력 텐서 x는 5개의 1\n",
    "y = torch.zeros(3)  # 예상 출력 y는 3개의 0\n",
    "w = torch.randn(5, 3, requires_grad=True)   # 가중치 w는 5x3 크기의 랜덤 텐서, 학습을 위해 미분 가능\n",
    "b = torch.randn(3, requires_grad=True)      # 편향 b는 3개의 랜덤 값, 학습을 위해 미분 가능\n",
    "z = torch.matmul(x, w) + b                  # 입력 x와 가중치 w를 행렬 곱한 후, 편향 b를 더함\n",
    "\n",
    "# 로짓 z와 예상 출력 y 사이의 이진 교차 엔트로피 손실 계산\n",
    "loss = torch.nn.functional.binary_cross_entropy_with_logits(z, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([1., 1., 1., 1., 1.]),\n",
       " tensor([0., 0., 0.]),\n",
       " tensor([[-1.0243,  0.2566, -0.9145],\n",
       "         [ 0.4071,  1.5941,  1.0600],\n",
       "         [-0.3276, -0.5151,  0.4659],\n",
       "         [ 1.2927, -0.0337,  0.5805],\n",
       "         [ 0.6181, -0.3403,  0.7627]], requires_grad=True),\n",
       " tensor([ 0.9699, -1.6834, -1.5001], requires_grad=True),\n",
       " tensor([ 1.9359, -0.7219,  0.4544]),\n",
       " tensor(1.1375, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x, y, w, b, z, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient function for z =  <AddBackward0 object at 0x00000216E940AB00>\n",
      "Gradient function for loss =  <BinaryCrossEntropyWithLogitsBackward0 object at 0x00000216CB093A00>\n"
     ]
    }
   ],
   "source": [
    "print('Gradient function for z = ', z.grad_fn)   # z 텐서의 기울기 함수(역전파 계산 함수)를 출력\n",
    "print('Gradient function for loss = ', loss.grad_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient(변화도) 계산\n",
    "- 신경망에서 매개변수의 가중치를 최적화하려면 매개변수에 대한 손실함수의 도함수(derivative)를 계산\n",
    "- ``x`` 와 ``y`` 의 일부 고정값에서 $\\frac{\\partial loss}{\\partial w}$와 $\\frac{\\partial loss}{\\partial b}$ 가 필요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0332, 0.0042, 0.3287],\n",
      "        [0.0332, 0.0042, 0.3287],\n",
      "        [0.0332, 0.0042, 0.3287],\n",
      "        [0.0332, 0.0042, 0.3287],\n",
      "        [0.0332, 0.0042, 0.3287]])\n",
      "tensor([0.0332, 0.0042, 0.3287])\n"
     ]
    }
   ],
   "source": [
    "loss.backward(retain_graph=True)        # 손실(loss)의 역전파를 계산\n",
    "# backward() 함수는 그래디언트를 계산하는데 사용, 자동 미분을 통해 손실 함수에 대한 모든 가중치의 기울기를 계산\n",
    "# retain_graph=True, 그래프가 메모리에 남아있어 후속 연산을 위한 추가적인 역전파 계산이 가능\n",
    "# 이 옵션은 한 번의 backward 호출로 여러 번의 역전파를 수행하려는 경우\n",
    "\n",
    "print(w.grad)\n",
    "print(b.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2913, 0.1090, 0.2039],\n",
      "        [0.2913, 0.1090, 0.2039],\n",
      "        [0.2913, 0.1090, 0.2039],\n",
      "        [0.2913, 0.1090, 0.2039],\n",
      "        [0.2913, 0.1090, 0.2039]])\n",
      "tensor([0.2913, 0.1090, 0.2039])\n"
     ]
    }
   ],
   "source": [
    "loss.backward(retain_graph=True)\n",
    "print(w.grad)\n",
    "print(b.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.5826, 0.2180, 0.4078],\n",
      "        [0.5826, 0.2180, 0.4078],\n",
      "        [0.5826, 0.2180, 0.4078],\n",
      "        [0.5826, 0.2180, 0.4078],\n",
      "        [0.5826, 0.2180, 0.4078]])\n",
      "tensor([0.5826, 0.2180, 0.4078])\n"
     ]
    }
   ],
   "source": [
    "loss.backward(retain_graph=True)\n",
    "print(w.grad)\n",
    "print(b.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- loss.backward() 가 호출이 될 때 마다 동일한 크기의 gradient 값이 추가로 가산됨"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 변화도 추적 멈추기\n",
    "- `requires_grad=True`인 모든 텐서들은 연산 기록을 추적하고 변화도 계산을 지원\n",
    "- 단, 모델을 학습한 뒤 입력 데이터를 단순히 적용하기만 하는 경우와 같이 순전파 연산만 필요한 경우, torch.no_grad() 활용\n",
    "- 동일한 결과를 얻는 다른 방법은 텐서에 `detach()` 메소드를 사용 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "z = torch.matmul(x, w)+b        # 입력 x와 가중치 w를 행렬 곱한 후, 편향 b를 더함\n",
    "print(z.requires_grad)          # z 텐서가 기울기를 추적해야 하는지 확인\n",
    "\n",
    "with torch.no_grad():           # 이 블록 내 텐서 연산이 기울기 추적 하지않게 설정\n",
    "  z = torch.matmul(x, w)+b      # 기울기 추적을 하지 않고 연산\n",
    "  print(z.requires_grad)        # 'no_grad' 블록 내에서 연산된 z는 기울기를 추적하지 않음, 따라서 False가 출력\n",
    "\n",
    "print(z.requires_grad)          # z는 'no_grad' 블록 밖에서 사용, 기울기 추적 여부가 False로 출력"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 선택적으로 읽기(Optional Reading): 텐서 변화도와 야코비안 곱(Jacobian Product)\n",
    "- 대부분의 경우, 스칼라 손실 함수를 가지고 일부 매개변수와 관련한 변화도를 계산해야 함\n",
    "- 그러나 출력 함수가 임의의 텐서인 경우가 있습니다. 이럴 때, PyTorch는 실제 변화도가 아닌 **야코비안 곱(Jacobian product)** 을 계산\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\vec{x}=\\langle x_1,\\dots,x_n\\rangle$ 이고, $\\vec{y}=\\langle y_1,\\dots,y_m\\rangle`$ 일 때 벡터 함수 $\\vec{y}=f(\\vec{x})$에서 $\\vec{x}$에 대한 $\\vec{y}$의 변화도는 **야코비안 행렬(Jacobian matrix)**로 제시:\n",
    "\n",
    "$$J=\\left(\\begin{array}{ccc}\n",
    "      \\frac{\\partial y_{1}}{\\partial x_{1}} & \\cdots & \\frac{\\partial y_{1}}{\\partial x_{n}}\\\\\n",
    "      \\vdots & \\ddots & \\vdots\\\\\n",
    "      \\frac{\\partial y_{m}}{\\partial x_{1}} & \\cdots & \\frac{\\partial y_{m}}{\\partial x_{n}}\n",
    "      \\end{array}\\right) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 야코비안 자체를 계산하는 대신, PyTorch는 주어진 입력 벡터 $v=(v_1 \\dots v_m)$ 에 대한 **야코비안 곱(Jacobian Product)**  $v^T\\cdot J$ 을 계산\n",
    "- 이 과정은 $v$ 를 인자로 ``backward`` 를 호출하면 끝. 단, $v$의 크기는 곱(product)을 계산하려고 하는 원래 텐서의 크기와 동일해야 함."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input tensor :\n",
      "tensor([[1., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 1., 0.],\n",
      "        [0., 0., 0., 0., 1.]], requires_grad=True)\n",
      "Output tensor :\n",
      "tensor([[4., 1., 1., 1., 1.],\n",
      "        [1., 4., 1., 1., 1.],\n",
      "        [1., 1., 4., 1., 1.],\n",
      "        [1., 1., 1., 4., 1.],\n",
      "        [1., 1., 1., 1., 4.]], grad_fn=<PowBackward0>)\n",
      "First call : \n",
      "tensor([[4., 2., 2., 2., 2.],\n",
      "        [2., 4., 2., 2., 2.],\n",
      "        [2., 2., 4., 2., 2.],\n",
      "        [2., 2., 2., 4., 2.],\n",
      "        [2., 2., 2., 2., 4.]])\n",
      "Second call : \n",
      "tensor([[8., 4., 4., 4., 4.],\n",
      "        [4., 8., 4., 4., 4.],\n",
      "        [4., 4., 8., 4., 4.],\n",
      "        [4., 4., 4., 8., 4.],\n",
      "        [4., 4., 4., 4., 8.]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp = torch.eye(5, requires_grad=True)  # 5x5 크기 단위 행렬을 생성, 기울기 추적 설정\n",
    "print(f'Input tensor :\\n{inp}')\n",
    "out = (inp + 1).pow(2)  # inp에 1을 더하고, 그 값을 제곱하여 out 텐서 생성\n",
    "print(f'Output tensor :\\n{out}')\n",
    "\n",
    "out.backward(torch.ones_like(inp), retain_graph=True)  # out에 대해 역전파 실행, 기울기 계산\n",
    "# torch.ones_like(inp)는 역전파 시 그래디언트의 초기값을 1로 설정\n",
    "print(f'First call : \\n{inp.grad}')  # 첫 번째 역전파 후 inp의 기울기 출력\n",
    "\n",
    "out.backward(torch.ones_like(inp), retain_graph=True)  # 같은 연산에 대해 다시 역전파 실행\n",
    "print(f'Second call : \\n{inp.grad}')  # 두 번째 역전파 후 inp의 기울기를 출력\n",
    "\n",
    "inp.grad.zero_()  # inp의 기울기 0으로 초기화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Call after zeroing gradients :\n",
      "tensor([[4., 2., 2., 2., 2.],\n",
      "        [2., 4., 2., 2., 2.],\n",
      "        [2., 2., 4., 2., 2.],\n",
      "        [2., 2., 2., 4., 2.],\n",
      "        [2., 2., 2., 2., 4.]])\n"
     ]
    }
   ],
   "source": [
    "out.backward(torch.ones_like(inp), retain_graph=True)  # 다시 역전파 실행\n",
    "print(f'Call after zeroing gradients :\\n{inp.grad}')  # 기울기 초기화 후 역전파 실행 후 inp의 기울기를 출력."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 역방향 전파를 수행할 때, PyTorch가 **변화도를 누적(accumulate)** 해 둠\n",
    "- (inp + 1).pow(2) 를 방정식으로 쓰면 $ y=(x+1)^ {2} $로 쓸수있고 gradient를 구하기 위해서 미분하면 $ y' = 2(x+1) $ 가 됨\n",
    "- x == inp 이고 inp는 5x5 크기 단위 행렬 이므로 이를 대입하면, First call 값이 나옴\n",
    "- 다시 역전파를 하면 inp에 다시 계산값이 더해져 Second call 값이 됨\n",
    "- 계속 누적이 되므로 grad는 초기화가 필요"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
