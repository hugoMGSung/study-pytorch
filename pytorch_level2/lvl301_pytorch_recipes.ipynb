{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch 학습 레벨 2 - 008\n",
    "---\n",
    "\n",
    "### PyTorch Recipes 01\n",
    "[링크](https://tutorials.pytorch.kr/recipes/recipes_index.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch에서 데이터 불러오기\n",
    "- [참조](https://tutorials.pytorch.kr/recipes/recipes/loading_data_recipe.html)\n",
    "- PyTorch는 인공신경망을 만드는데 필요한 다양한 기본 요소를 간단하고 직관적이며 안정적인 API로 제공합니다. PyTorch는 공용 데이터셋을 쉽게 사용할 수 있도록 도와주는 패키지를 포함하고 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 개요\n",
    "\n",
    "PyTorch 데이터 불러오기 기능의 핵심은 [torch.utils.data.DataLoader](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader) 클래스입니다. 데이터를 파이썬 iterable로써 접근할 수 있게 해주는 클래스입니다. 또한, [torch.utils.data.Dataset](https://pytorch.org/docs/stable/data.html#torch.utils.data.Dataset) 클래스를 통해 PyTorch에 내장된 다양한 고품질 데이터셋을 이용하실 수 있습니다.\n",
    "\n",
    "개별 데이터셋은 아래 패키지에서 확인하실 수 있으며, 데이터셋은 계속해서 추가될 예정입니다.\n",
    "\n",
    "- [torchvision](https://pytorch.org/vision/stable/datasets.html)\n",
    "- [torchaudio](https://pytorch.org/audio/stable/datasets.html)\n",
    "- [torchtext](https://pytorch.org/text/stable/datasets.html)\n",
    "\n",
    "이번 레시피에서는 `torchaudio.datasets.YESNO` 데이터셋을 살펴보면서, PyTorch `Dataset` 에서 PyTorch `DataLoader` 로 데이터를 효과적이고 효율적으로 불러오는 방법을 살펴보겠습니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 초기 설정(Setup)\n",
    "\n",
    "시작하기 전에, 데이터셋이 포함된 `torchaudio` 패키지를 설치합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 단계(Steps)\n",
    "\n",
    "1. 데이터를 불러오는데 필요한 라이브러리 import하기\n",
    "\n",
    "2. 데이터 접근하기\n",
    "\n",
    "3. 데이터 불러오기\n",
    "\n",
    "4. 데이터 순회하기\n",
    "\n",
    "5. [선택 사항] 데이터 시각화하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1. 데이터를 불러오는데 필요한 라이브러리 import하기\n",
    "\n",
    "이번 레시피는 `torch` 와 `torchaudio` 를 사용합니다. 다른 내장 데이터셋이 필요하다면 `torchvision` 혹은 `torchtext` 를 설치해서 사용해도 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torchaudio in c:\\users\\perso\\.pyenv\\pyenv-win\\versions\\3.11.9\\lib\\site-packages (2.5.1+cu124)\n",
      "Requirement already satisfied: torch==2.5.1+cu124 in c:\\users\\perso\\.pyenv\\pyenv-win\\versions\\3.11.9\\lib\\site-packages (from torchaudio) (2.5.1+cu124)\n",
      "Requirement already satisfied: filelock in c:\\users\\perso\\.pyenv\\pyenv-win\\versions\\3.11.9\\lib\\site-packages (from torch==2.5.1+cu124->torchaudio) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\perso\\.pyenv\\pyenv-win\\versions\\3.11.9\\lib\\site-packages (from torch==2.5.1+cu124->torchaudio) (4.9.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\perso\\.pyenv\\pyenv-win\\versions\\3.11.9\\lib\\site-packages (from torch==2.5.1+cu124->torchaudio) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\perso\\.pyenv\\pyenv-win\\versions\\3.11.9\\lib\\site-packages (from torch==2.5.1+cu124->torchaudio) (3.1.3)\n",
      "Requirement already satisfied: fsspec in c:\\users\\perso\\.pyenv\\pyenv-win\\versions\\3.11.9\\lib\\site-packages (from torch==2.5.1+cu124->torchaudio) (2024.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\perso\\.pyenv\\pyenv-win\\versions\\3.11.9\\lib\\site-packages (from torch==2.5.1+cu124->torchaudio) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\perso\\.pyenv\\pyenv-win\\versions\\3.11.9\\lib\\site-packages (from sympy==1.13.1->torch==2.5.1+cu124->torchaudio) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\perso\\.pyenv\\pyenv-win\\versions\\3.11.9\\lib\\site-packages (from jinja2->torch==2.5.1+cu124->torchaudio) (2.1.5)\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sox\n",
      "  Downloading sox-1.5.0.tar.gz (63 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: numpy>=1.9.0 in c:\\users\\perso\\.pyenv\\pyenv-win\\versions\\3.11.9\\lib\\site-packages (from sox) (1.26.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.2 in c:\\users\\perso\\.pyenv\\pyenv-win\\versions\\3.11.9\\lib\\site-packages (from sox) (4.9.0)\n",
      "Building wheels for collected packages: sox\n",
      "  Building wheel for sox (setup.py): started\n",
      "  Building wheel for sox (setup.py): finished with status 'done'\n",
      "  Created wheel for sox: filename=sox-1.5.0-py3-none-any.whl size=40071 sha256=5bed04fead87e9046423f2924e36082474896dbac5b192c940678d5c53e12b17\n",
      "  Stored in directory: c:\\users\\perso\\appdata\\local\\pip\\cache\\wheels\\74\\89\\93\\023fcdacaec4e5471e78b43992515e8500cc2505b307e2e6b7\n",
      "Successfully built sox\n",
      "Installing collected packages: sox\n",
      "Successfully installed sox-1.5.0\n"
     ]
    }
   ],
   "source": [
    "# 추가 설치\n",
    "!pip install sox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\perso\\AppData\\Local\\Temp\\ipykernel_14232\\3159160044.py:2: UserWarning: torchaudio._backend.set_audio_backend has been deprecated. With dispatcher enabled, this function is no-op. You can remove the function call.\n",
      "  torchaudio.set_audio_backend(\"sox_io\")  # sox_io 백엔드를 명시적으로 사용\n"
     ]
    }
   ],
   "source": [
    "## 필요없음\n",
    "torchaudio.set_audio_backend(\"sox_io\")  # sox_io 백엔드를 명시적으로 사용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2. 데이터에 접근하기\n",
    "\n",
    "`torchaudio` 의 `yesno` 데이터셋은 한 사람이 히브리어로 yes 혹은 no를 녹음한 오디오 클립 60개로 구성되어 있습니다. 오디오 클립 각각의 길이는 단어 8개입니다. ( [더 알아보기](https://www.openslr.org/1/) ).\n",
    "\n",
    "`torchaudio.datasets.YESNO` 클래스를 사용하여 `yesno` 데이터셋을 생성합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4.49M/4.49M [00:02<00:00, 1.98MB/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torchaudio.datasets.yesno.YESNO at 0x25218bbcfd0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torchaudio.datasets.YESNO(\n",
    "     root='./data',\n",
    "     url='http://www.openslr.org/resources/1/waves_yesno.tar.gz',\n",
    "     folder_in_archive='waves_yesno',\n",
    "     download=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "각각의 데이터 항목 (item)은 튜플 형태 (waveform: 파형, sample_rate: 샘플 속도, labels: 라벨)를 갖습니다.\n",
    "\n",
    "`yesno` 데이터셋을 불러올 때 `root` 매개변수는 꼭 지정해주셔야 합니다. `root` 는 학습(training) 및 테스트(testing) 데이터셋이 존재하는 위치를 가르켜야 합니다. 그 외의 매개변수는 선택 사항이며, 위 예시에서 기본값을 확인하실 있습니다. 아래와 같은 매개변수도 사용 가능합니다.\n",
    "\n",
    "`download`: 참(True)인 경우, 데이터셋 파일을 인터넷에서 다운받고 `root` 폴더에 저장합니다. 파일이 이미 존재하면 다시 다운받지 않습니다.\n",
    "\n",
    "이제 `yesno` 데이터를 확인해봅시다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Couldn't find appropriate backend to handle uri data\\waves_yesno\\0_0_1_0_0_0_1_0.wav and format None.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# 실제 데이터에 접근해서 ``yesno_data`` 의 형태를 확인합니다. 세 번째 항목을 예시로 살펴봅니다.\u001b[39;00m\n\u001b[0;32m      6\u001b[0m n \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m3\u001b[39m\n\u001b[1;32m----> 7\u001b[0m waveform, sample_rate, labels \u001b[38;5;241m=\u001b[39m \u001b[43myesno_data\u001b[49m\u001b[43m[\u001b[49m\u001b[43mn\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWaveform: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mSample rate: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mLabels: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(waveform, sample_rate, labels))\n",
      "File \u001b[1;32mc:\\Users\\perso\\.pyenv\\pyenv-win\\versions\\3.11.9\\Lib\\site-packages\\torchaudio\\datasets\\yesno.py:85\u001b[0m, in \u001b[0;36mYESNO.__getitem__\u001b[1;34m(self, n)\u001b[0m\n\u001b[0;32m     69\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Load the n-th sample from the dataset.\u001b[39;00m\n\u001b[0;32m     70\u001b[0m \n\u001b[0;32m     71\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     82\u001b[0m \u001b[38;5;124;03m        labels\u001b[39;00m\n\u001b[0;32m     83\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     84\u001b[0m fileid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_walker[n]\n\u001b[1;32m---> 85\u001b[0m item \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load_item\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfileid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     86\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m item\n",
      "File \u001b[1;32mc:\\Users\\perso\\.pyenv\\pyenv-win\\versions\\3.11.9\\Lib\\site-packages\\torchaudio\\datasets\\yesno.py:65\u001b[0m, in \u001b[0;36mYESNO._load_item\u001b[1;34m(self, fileid, path)\u001b[0m\n\u001b[0;32m     63\u001b[0m labels \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mint\u001b[39m(c) \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m fileid\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m)]\n\u001b[0;32m     64\u001b[0m file_audio \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(path, fileid \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.wav\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 65\u001b[0m waveform, sample_rate \u001b[38;5;241m=\u001b[39m \u001b[43mtorchaudio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_audio\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m waveform, sample_rate, labels\n",
      "File \u001b[1;32mc:\\Users\\perso\\.pyenv\\pyenv-win\\versions\\3.11.9\\Lib\\site-packages\\torchaudio\\_backend\\utils.py:204\u001b[0m, in \u001b[0;36mget_load_func.<locals>.load\u001b[1;34m(uri, frame_offset, num_frames, normalize, channels_first, format, buffer_size, backend)\u001b[0m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload\u001b[39m(\n\u001b[0;32m    119\u001b[0m     uri: Union[BinaryIO, \u001b[38;5;28mstr\u001b[39m, os\u001b[38;5;241m.\u001b[39mPathLike],\n\u001b[0;32m    120\u001b[0m     frame_offset: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    126\u001b[0m     backend: Optional[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    127\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor, \u001b[38;5;28mint\u001b[39m]:\n\u001b[0;32m    128\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Load audio data from source.\u001b[39;00m\n\u001b[0;32m    129\u001b[0m \n\u001b[0;32m    130\u001b[0m \u001b[38;5;124;03m    By default (``normalize=True``, ``channels_first=True``), this function returns Tensor with\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    202\u001b[0m \u001b[38;5;124;03m            `[channel, time]` else `[time, channel]`.\u001b[39;00m\n\u001b[0;32m    203\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 204\u001b[0m     backend \u001b[38;5;241m=\u001b[39m \u001b[43mdispatcher\u001b[49m\u001b[43m(\u001b[49m\u001b[43muri\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbackend\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    205\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m backend\u001b[38;5;241m.\u001b[39mload(uri, frame_offset, num_frames, normalize, channels_first, \u001b[38;5;28mformat\u001b[39m, buffer_size)\n",
      "File \u001b[1;32mc:\\Users\\perso\\.pyenv\\pyenv-win\\versions\\3.11.9\\Lib\\site-packages\\torchaudio\\_backend\\utils.py:116\u001b[0m, in \u001b[0;36mget_load_func.<locals>.dispatcher\u001b[1;34m(uri, format, backend_name)\u001b[0m\n\u001b[0;32m    114\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m backend\u001b[38;5;241m.\u001b[39mcan_decode(uri, \u001b[38;5;28mformat\u001b[39m):\n\u001b[0;32m    115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m backend\n\u001b[1;32m--> 116\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCouldn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt find appropriate backend to handle uri \u001b[39m\u001b[38;5;132;01m{\u001b[39;00muri\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and format \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mformat\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Couldn't find appropriate backend to handle uri data\\waves_yesno\\0_0_1_0_0_0_1_0.wav and format None."
     ]
    }
   ],
   "source": [
    "# ``yesno`` 안에 각각의 데이터 항목은 튜플 형태 (파형, 샘플 속도, 라벨)를 가지며,\n",
    "# 이때 labels는 0(no)과 1(yes)을 담은 리스트 형태로 되어 있습니다.\n",
    "yesno_data = torchaudio.datasets.YESNO('./data', download=True)\n",
    "\n",
    "# 실제 데이터에 접근해서 ``yesno_data`` 의 형태를 확인합니다. 세 번째 항목을 예시로 살펴봅니다.\n",
    "n = 3\n",
    "waveform, sample_rate, labels = yesno_data[n]\n",
    "print(\"Waveform: {}\\nSample rate: {}\\nLabels: {}\".format(waveform, sample_rate, labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `uri data\\waves_yesno\\0_0_1_0_0_0_1_0.wav를 처리하고 None 형식을 지정할 적절한 백엔드를 찾을 수 없습니다.` 라는 RuntimeError 발생. 계속 진행 안됨"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    <p>아래로 계속 진행 안됩니다!</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "실제 상황에서는 데이터를 《학습(training)》 데이터셋과 《테스트(testing)》 데이터셋으로 나누는 것이 권장됩니다. 모델의 성능을 제대로 평가하려면 학습에 쓰이지 않은 out-of-sample 데이터를 이용해야 하기 때문입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3. 데이터 불러오기\n",
    "\n",
    "데이터셋에 성공적으로 접근했으니, 이제 데이터셋을 `torch.utils.data.DataLoader` 로 넘겨줍니다. `DataLoader` 는 데이터셋을 `sampler와` 조합시켜 데이터셋을 순회할 수 있는 iterable을 만들어줍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = torch.utils.data.DataLoader(yesno_data,\n",
    "                                          batch_size=1,\n",
    "                                          shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4. 데이터 순회하기\n",
    "\n",
    "이제 `data_loader` 를 이용해서 데이터를 순회할 수 있습니다. 모델을 학습하려면 이처럼 데이터를 순회할 수 있어야 합니다. 아래 예시를 보시면 `data_loader` 안에 있는 각각의 데이터 항목이 파형, 샘플 속도, 라벨을 담은 텐서로 바뀌었음을 확인할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for data in data_loader:\n",
    "  print(\"Data: \", data)\n",
    "  print(\"Waveform: {}\\nSample rate: {}\\nLabels: {}\".format(data[0], data[1], data[2]))\n",
    "  break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5. [선택 사항] 데이터 시각화하기\n",
    "\n",
    "`DataLoader` 의 데이터를 시각화해서 더 자세히 확인해보실 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mdata\u001b[49m[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mnumpy())\n\u001b[0;32m      5\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure()\n\u001b[0;32m      6\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(waveform\u001b[38;5;241m.\u001b[39mt()\u001b[38;5;241m.\u001b[39mnumpy())\n",
      "\u001b[1;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(data[0][0].numpy())\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(waveform.t().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "축하드립니다! `PyTorch`에서 데이터를 불러오는데 성공하셨습니다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
