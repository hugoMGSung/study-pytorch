{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch lighting\n",
    "\n",
    "#### PyTorch lighting\n",
    "머신러닝, 딥러닝 모델 구축을 할 때에 공통된 부분들을 반복해서 작성할 필요 없이 대신 처리해주고, 머신러닝 모델 구축의 탬플릿 코드로써 기능을 하며, 다른 사람이 작성한 코드를 쉽게 볼 수 있도록 공통된 스타일을 갖도록 하고 ,모델의 개별적인 부분은 유연하게 커스터마이징하여 실험할 수 있는 라이브러리\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/2000/1*-GDzOk_UJElhGtnc9g6zHA.png\" width=\"700\">\n",
    "\n",
    "#### 설치"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pytorch-lightning\n",
      "  Using cached pytorch_lightning-2.5.0.post0-py3-none-any.whl.metadata (21 kB)\n",
      "Requirement already satisfied: torch>=2.1.0 in c:\\users\\perso\\.pyenv\\pyenv-win\\versions\\3.11.9\\lib\\site-packages (from pytorch-lightning) (2.5.1+cu124)\n",
      "Requirement already satisfied: tqdm>=4.57.0 in c:\\users\\perso\\.pyenv\\pyenv-win\\versions\\3.11.9\\lib\\site-packages (from pytorch-lightning) (4.66.5)\n",
      "Requirement already satisfied: PyYAML>=5.4 in c:\\users\\perso\\.pyenv\\pyenv-win\\versions\\3.11.9\\lib\\site-packages (from pytorch-lightning) (6.0.2)\n",
      "Requirement already satisfied: fsspec>=2022.5.0 in c:\\users\\perso\\.pyenv\\pyenv-win\\versions\\3.11.9\\lib\\site-packages (from fsspec[http]>=2022.5.0->pytorch-lightning) (2024.2.0)\n",
      "Collecting torchmetrics>=0.7.0 (from pytorch-lightning)\n",
      "  Using cached torchmetrics-1.6.1-py3-none-any.whl.metadata (21 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\perso\\.pyenv\\pyenv-win\\versions\\3.11.9\\lib\\site-packages (from pytorch-lightning) (24.2)\n",
      "Requirement already satisfied: typing-extensions>=4.4.0 in c:\\users\\perso\\.pyenv\\pyenv-win\\versions\\3.11.9\\lib\\site-packages (from pytorch-lightning) (4.9.0)\n",
      "Collecting lightning-utilities>=0.10.0 (from pytorch-lightning)\n",
      "  Using cached lightning_utilities-0.11.9-py3-none-any.whl.metadata (5.2 kB)\n",
      "Collecting aiohttp!=4.0.0a0,!=4.0.0a1 (from fsspec[http]>=2022.5.0->pytorch-lightning)\n",
      "  Downloading aiohttp-3.11.11-cp311-cp311-win_amd64.whl.metadata (8.0 kB)\n",
      "Requirement already satisfied: setuptools in c:\\users\\perso\\.pyenv\\pyenv-win\\versions\\3.11.9\\lib\\site-packages (from lightning-utilities>=0.10.0->pytorch-lightning) (75.6.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\perso\\.pyenv\\pyenv-win\\versions\\3.11.9\\lib\\site-packages (from torch>=2.1.0->pytorch-lightning) (3.13.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\perso\\.pyenv\\pyenv-win\\versions\\3.11.9\\lib\\site-packages (from torch>=2.1.0->pytorch-lightning) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\perso\\.pyenv\\pyenv-win\\versions\\3.11.9\\lib\\site-packages (from torch>=2.1.0->pytorch-lightning) (3.1.3)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\perso\\.pyenv\\pyenv-win\\versions\\3.11.9\\lib\\site-packages (from torch>=2.1.0->pytorch-lightning) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\perso\\.pyenv\\pyenv-win\\versions\\3.11.9\\lib\\site-packages (from sympy==1.13.1->torch>=2.1.0->pytorch-lightning) (1.3.0)\n",
      "Requirement already satisfied: numpy>1.20.0 in c:\\users\\perso\\.pyenv\\pyenv-win\\versions\\3.11.9\\lib\\site-packages (from torchmetrics>=0.7.0->pytorch-lightning) (1.26.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\perso\\.pyenv\\pyenv-win\\versions\\3.11.9\\lib\\site-packages (from tqdm>=4.57.0->pytorch-lightning) (0.4.6)\n",
      "Collecting aiohappyeyeballs>=2.3.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning)\n",
      "  Using cached aiohappyeyeballs-2.4.4-py3-none-any.whl.metadata (6.1 kB)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning)\n",
      "  Using cached aiosignal-1.3.2-py2.py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting attrs>=17.3.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning)\n",
      "  Downloading attrs-25.1.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning)\n",
      "  Downloading frozenlist-1.5.0-cp311-cp311-win_amd64.whl.metadata (14 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning)\n",
      "  Downloading multidict-6.1.0-cp311-cp311-win_amd64.whl.metadata (5.1 kB)\n",
      "Collecting propcache>=0.2.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning)\n",
      "  Downloading propcache-0.2.1-cp311-cp311-win_amd64.whl.metadata (9.5 kB)\n",
      "Collecting yarl<2.0,>=1.17.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning)\n",
      "  Downloading yarl-1.18.3-cp311-cp311-win_amd64.whl.metadata (71 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\perso\\.pyenv\\pyenv-win\\versions\\3.11.9\\lib\\site-packages (from jinja2->torch>=2.1.0->pytorch-lightning) (2.1.5)\n",
      "Requirement already satisfied: idna>=2.0 in c:\\users\\perso\\.pyenv\\pyenv-win\\versions\\3.11.9\\lib\\site-packages (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (3.4)\n",
      "Using cached pytorch_lightning-2.5.0.post0-py3-none-any.whl (819 kB)\n",
      "Using cached lightning_utilities-0.11.9-py3-none-any.whl (28 kB)\n",
      "Using cached torchmetrics-1.6.1-py3-none-any.whl (927 kB)\n",
      "Downloading aiohttp-3.11.11-cp311-cp311-win_amd64.whl (442 kB)\n",
      "Using cached aiohappyeyeballs-2.4.4-py3-none-any.whl (14 kB)\n",
      "Using cached aiosignal-1.3.2-py2.py3-none-any.whl (7.6 kB)\n",
      "Downloading attrs-25.1.0-py3-none-any.whl (63 kB)\n",
      "Downloading frozenlist-1.5.0-cp311-cp311-win_amd64.whl (51 kB)\n",
      "Downloading multidict-6.1.0-cp311-cp311-win_amd64.whl (28 kB)\n",
      "Downloading propcache-0.2.1-cp311-cp311-win_amd64.whl (44 kB)\n",
      "Downloading yarl-1.18.3-cp311-cp311-win_amd64.whl (91 kB)\n",
      "Installing collected packages: propcache, multidict, lightning-utilities, frozenlist, attrs, aiohappyeyeballs, yarl, aiosignal, torchmetrics, aiohttp, pytorch-lightning\n",
      "Successfully installed aiohappyeyeballs-2.4.4 aiohttp-3.11.11 aiosignal-1.3.2 attrs-25.1.0 frozenlist-1.5.0 lightning-utilities-0.11.9 multidict-6.1.0 propcache-0.2.1 pytorch-lightning-2.5.0.post0 torchmetrics-1.6.1 yarl-1.18.3\n"
     ]
    }
   ],
   "source": [
    "!pip install pytorch-lightning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### PyTorch Lightning을 사용하여 딥러닝 모델을 작성하는 순서\n",
    "\n",
    "1. Lightning Module에서 상속된 새로운 Lightning Module 클래스를 작성\n",
    "2. DataLoader를 통해 학습할 데이터를 준비\n",
    "3. Trainer 객체를 만들고, 그 Trainer에 데이터와 Lightning Module 클래스를 주어 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sympy in c:\\users\\perso\\.pyenv\\pyenv-win\\versions\\3.11.9\\lib\\site-packages (1.13.1)\n",
      "Collecting sympy\n",
      "  Using cached sympy-1.13.3-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\perso\\.pyenv\\pyenv-win\\versions\\3.11.9\\lib\\site-packages (from sympy) (1.3.0)\n",
      "Using cached sympy-1.13.3-py3-none-any.whl (6.2 MB)\n",
      "Installing collected packages: sympy\n",
      "  Attempting uninstall: sympy\n",
      "    Found existing installation: sympy 1.13.1\n",
      "    Uninstalling sympy-1.13.1:\n",
      "      Successfully uninstalled sympy-1.13.1\n",
      "Successfully installed sympy-1.13.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "torch 2.5.1+cu124 requires sympy==1.13.1; python_version >= \"3.9\", but you have sympy 1.13.3 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade sympy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sympy==1.13.1\n",
      "  Using cached sympy-1.13.1-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\perso\\.pyenv\\pyenv-win\\versions\\3.11.9\\lib\\site-packages (from sympy==1.13.1) (1.3.0)\n",
      "Using cached sympy-1.13.1-py3-none-any.whl (6.2 MB)\n",
      "Installing collected packages: sympy\n",
      "  Attempting uninstall: sympy\n",
      "    Found existing installation: sympy 1.13.3\n",
      "    Uninstalling sympy-1.13.3:\n",
      "      Successfully uninstalled sympy-1.13.3\n",
      "Successfully installed sympy-1.13.1\n"
     ]
    }
   ],
   "source": [
    "!pip install sympy==1.13.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "<urlopen error [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다>\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to d:\\01_Programming\\100_HugoBank\\Mine\\study-pytorch\\pytorch_chatbot\\MNIST\\raw\\train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9.91M/9.91M [00:12<00:00, 788kB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting d:\\01_Programming\\100_HugoBank\\Mine\\study-pytorch\\pytorch_chatbot\\MNIST\\raw\\train-images-idx3-ubyte.gz to d:\\01_Programming\\100_HugoBank\\Mine\\study-pytorch\\pytorch_chatbot\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "<urlopen error [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다>\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to d:\\01_Programming\\100_HugoBank\\Mine\\study-pytorch\\pytorch_chatbot\\MNIST\\raw\\train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28.9k/28.9k [00:00<00:00, 154kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting d:\\01_Programming\\100_HugoBank\\Mine\\study-pytorch\\pytorch_chatbot\\MNIST\\raw\\train-labels-idx1-ubyte.gz to d:\\01_Programming\\100_HugoBank\\Mine\\study-pytorch\\pytorch_chatbot\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "<urlopen error [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다>\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to d:\\01_Programming\\100_HugoBank\\Mine\\study-pytorch\\pytorch_chatbot\\MNIST\\raw\\t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1.65M/1.65M [00:00<00:00, 1.72MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting d:\\01_Programming\\100_HugoBank\\Mine\\study-pytorch\\pytorch_chatbot\\MNIST\\raw\\t10k-images-idx3-ubyte.gz to d:\\01_Programming\\100_HugoBank\\Mine\\study-pytorch\\pytorch_chatbot\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "<urlopen error [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다>\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to d:\\01_Programming\\100_HugoBank\\Mine\\study-pytorch\\pytorch_chatbot\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4.54k/4.54k [00:00<00:00, 4.54MB/s]\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "d:\\01_Programming\\100_HugoBank\\Mine\\study-pytorch\\torch-cpu-env\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\logger_connector\\logger_connector.py:76: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `pytorch_lightning` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default\n",
      "d:\\01_Programming\\100_HugoBank\\Mine\\study-pytorch\\torch-cpu-env\\Lib\\site-packages\\pytorch_lightning\\loops\\utilities.py:73: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "\n",
      "  | Name | Type   | Params | Mode \n",
      "----------------------------------------\n",
      "0 | l1   | Linear | 7.9 K  | train\n",
      "----------------------------------------\n",
      "7.9 K     Trainable params\n",
      "0         Non-trainable params\n",
      "7.9 K     Total params\n",
      "0.031     Total estimated model params size (MB)\n",
      "1         Modules in train mode\n",
      "0         Modules in eval mode\n",
      "d:\\01_Programming\\100_HugoBank\\Mine\\study-pytorch\\torch-cpu-env\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting d:\\01_Programming\\100_HugoBank\\Mine\\study-pytorch\\pytorch_chatbot\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz to d:\\01_Programming\\100_HugoBank\\Mine\\study-pytorch\\pytorch_chatbot\\MNIST\\raw\n",
      "\n",
      "Epoch 1:  18%|█▊        | 11026/60000 [00:49<03:38, 224.26it/s, v_num=0]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Detected KeyboardInterrupt, attempting graceful shutdown ...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'exit' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[1;32md:\\01_Programming\\100_HugoBank\\Mine\\study-pytorch\\torch-cpu-env\\Lib\\site-packages\\pytorch_lightning\\trainer\\call.py:47\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[1;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[0;32m     46\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher\u001b[38;5;241m.\u001b[39mlaunch(trainer_fn, \u001b[38;5;241m*\u001b[39margs, trainer\u001b[38;5;241m=\u001b[39mtrainer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m---> 47\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrainer_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _TunerExitException:\n",
      "File \u001b[1;32md:\\01_Programming\\100_HugoBank\\Mine\\study-pytorch\\torch-cpu-env\\Lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:575\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[1;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[0;32m    569\u001b[0m ckpt_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_checkpoint_connector\u001b[38;5;241m.\u001b[39m_select_ckpt_path(\n\u001b[0;32m    570\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn,\n\u001b[0;32m    571\u001b[0m     ckpt_path,\n\u001b[0;32m    572\u001b[0m     model_provided\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    573\u001b[0m     model_connected\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    574\u001b[0m )\n\u001b[1;32m--> 575\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mckpt_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    577\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstopped\n",
      "File \u001b[1;32md:\\01_Programming\\100_HugoBank\\Mine\\study-pytorch\\torch-cpu-env\\Lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:982\u001b[0m, in \u001b[0;36mTrainer._run\u001b[1;34m(self, model, ckpt_path)\u001b[0m\n\u001b[0;32m    979\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[0;32m    980\u001b[0m \u001b[38;5;66;03m# RUN THE TRAINER\u001b[39;00m\n\u001b[0;32m    981\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m--> 982\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_stage\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    984\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[0;32m    985\u001b[0m \u001b[38;5;66;03m# POST-Training CLEAN UP\u001b[39;00m\n\u001b[0;32m    986\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n",
      "File \u001b[1;32md:\\01_Programming\\100_HugoBank\\Mine\\study-pytorch\\torch-cpu-env\\Lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1026\u001b[0m, in \u001b[0;36mTrainer._run_stage\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1025\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mset_detect_anomaly(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_detect_anomaly):\n\u001b[1;32m-> 1026\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1027\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32md:\\01_Programming\\100_HugoBank\\Mine\\study-pytorch\\torch-cpu-env\\Lib\\site-packages\\pytorch_lightning\\loops\\fit_loop.py:216\u001b[0m, in \u001b[0;36m_FitLoop.run\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_start()\n\u001b[1;32m--> 216\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madvance\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    217\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_end()\n",
      "File \u001b[1;32md:\\01_Programming\\100_HugoBank\\Mine\\study-pytorch\\torch-cpu-env\\Lib\\site-packages\\pytorch_lightning\\loops\\fit_loop.py:455\u001b[0m, in \u001b[0;36m_FitLoop.advance\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    454\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data_fetcher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 455\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mepoch_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data_fetcher\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\01_Programming\\100_HugoBank\\Mine\\study-pytorch\\torch-cpu-env\\Lib\\site-packages\\pytorch_lightning\\loops\\training_epoch_loop.py:150\u001b[0m, in \u001b[0;36m_TrainingEpochLoop.run\u001b[1;34m(self, data_fetcher)\u001b[0m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madvance\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_fetcher\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    151\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_end(data_fetcher)\n",
      "File \u001b[1;32md:\\01_Programming\\100_HugoBank\\Mine\\study-pytorch\\torch-cpu-env\\Lib\\site-packages\\pytorch_lightning\\loops\\training_epoch_loop.py:282\u001b[0m, in \u001b[0;36m_TrainingEpochLoop.advance\u001b[1;34m(self, data_fetcher)\u001b[0m\n\u001b[0;32m    281\u001b[0m dataloader_iter \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 282\u001b[0m batch, _, __ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(data_fetcher)\n\u001b[0;32m    283\u001b[0m \u001b[38;5;66;03m# TODO: we should instead use the batch_idx returned by the fetcher, however, that will require saving the\u001b[39;00m\n\u001b[0;32m    284\u001b[0m \u001b[38;5;66;03m# fetcher state so that the batch_idx is correct after restarting\u001b[39;00m\n",
      "File \u001b[1;32md:\\01_Programming\\100_HugoBank\\Mine\\study-pytorch\\torch-cpu-env\\Lib\\site-packages\\pytorch_lightning\\loops\\fetchers.py:134\u001b[0m, in \u001b[0;36m_PrefetchDataFetcher.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    132\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdone:\n\u001b[0;32m    133\u001b[0m     \u001b[38;5;66;03m# this will run only when no pre-fetching was done.\u001b[39;00m\n\u001b[1;32m--> 134\u001b[0m     batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__next__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    135\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    136\u001b[0m     \u001b[38;5;66;03m# the iterator is empty\u001b[39;00m\n",
      "File \u001b[1;32md:\\01_Programming\\100_HugoBank\\Mine\\study-pytorch\\torch-cpu-env\\Lib\\site-packages\\pytorch_lightning\\loops\\fetchers.py:61\u001b[0m, in \u001b[0;36m_DataFetcher.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     60\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 61\u001b[0m     batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miterator)\n\u001b[0;32m     62\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n",
      "File \u001b[1;32md:\\01_Programming\\100_HugoBank\\Mine\\study-pytorch\\torch-cpu-env\\Lib\\site-packages\\pytorch_lightning\\utilities\\combined_loader.py:341\u001b[0m, in \u001b[0;36mCombinedLoader.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    340\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterator \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 341\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterator)\n\u001b[0;32m    342\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterator, _Sequential):\n",
      "File \u001b[1;32md:\\01_Programming\\100_HugoBank\\Mine\\study-pytorch\\torch-cpu-env\\Lib\\site-packages\\pytorch_lightning\\utilities\\combined_loader.py:78\u001b[0m, in \u001b[0;36m_MaxSizeCycle.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     77\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 78\u001b[0m     out[i] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miterators[i])\n\u001b[0;32m     79\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n",
      "File \u001b[1;32md:\\01_Programming\\100_HugoBank\\Mine\\study-pytorch\\torch-cpu-env\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:701\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 701\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    702\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[1;32md:\\01_Programming\\100_HugoBank\\Mine\\study-pytorch\\torch-cpu-env\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:757\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    756\u001b[0m index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 757\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    758\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n",
      "File \u001b[1;32md:\\01_Programming\\100_HugoBank\\Mine\\study-pytorch\\torch-cpu-env\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32md:\\01_Programming\\100_HugoBank\\Mine\\study-pytorch\\torch-cpu-env\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32md:\\01_Programming\\100_HugoBank\\Mine\\study-pytorch\\torch-cpu-env\\Lib\\site-packages\\torchvision\\datasets\\mnist.py:139\u001b[0m, in \u001b[0;36mMNIST.__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m    132\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    133\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m    134\u001b[0m \u001b[38;5;124;03m    index (int): Index\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    137\u001b[0m \u001b[38;5;124;03m    tuple: (image, target) where target is index of the target class.\u001b[39;00m\n\u001b[0;32m    138\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m--> 139\u001b[0m img, target \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata[index], \u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtargets\u001b[49m\u001b[43m[\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    141\u001b[0m \u001b[38;5;66;03m# doing this so that it is consistent with all other datasets\u001b[39;00m\n\u001b[0;32m    142\u001b[0m \u001b[38;5;66;03m# to return a PIL Image\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 33\u001b[0m\n\u001b[0;32m     31\u001b[0m trainer \u001b[38;5;241m=\u001b[39m pl\u001b[38;5;241m.\u001b[39mTrainer()\n\u001b[0;32m     32\u001b[0m model \u001b[38;5;241m=\u001b[39m LitModel()\n\u001b[1;32m---> 33\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\01_Programming\\100_HugoBank\\Mine\\study-pytorch\\torch-cpu-env\\Lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:539\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[1;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[0;32m    537\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m=\u001b[39m TrainerStatus\u001b[38;5;241m.\u001b[39mRUNNING\n\u001b[0;32m    538\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m--> 539\u001b[0m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_and_handle_interrupt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    540\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_impl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\n\u001b[0;32m    541\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\01_Programming\\100_HugoBank\\Mine\\study-pytorch\\torch-cpu-env\\Lib\\site-packages\\pytorch_lightning\\trainer\\call.py:64\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[1;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(launcher, _SubprocessScriptLauncher):\n\u001b[0;32m     63\u001b[0m         launcher\u001b[38;5;241m.\u001b[39mkill(_get_sigkill_signal())\n\u001b[1;32m---> 64\u001b[0m     \u001b[43mexit\u001b[49m(\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exception:\n\u001b[0;32m     67\u001b[0m     _interrupt(trainer, exception)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'exit' is not defined"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import MNIST\n",
    "import os\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "class LitModel(pl.LightningModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.l1 = nn.Linear(28 * 28, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.relu(self.l1(x.view(x.size(0), -1)))\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = F.cross_entropy(y_hat, y)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=0.02)\n",
    "\n",
    "train_loader = DataLoader(MNIST(os.getcwd(), download=True, transform=transforms.ToTensor()))\n",
    "\n",
    "trainer = pl.Trainer()\n",
    "model = LitModel()\n",
    "trainer.fit(model, train_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LightningModule Class\n",
    "\n",
    "trainer와 모델이 상호작용을 할 수 있도록 pytorch의 nn.Module의 상위 클래스인 lightning module을 구현\n",
    "- 기존 PyTorch는 DataLoader, Mode, optimizer, Training loof 등을 전부 따로따로 코드로 구현\n",
    "- Pytorch Lightning에서는 Lightning Model class 안에 이 모든것을 한번에 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이 형태\n",
    "class Classifier(pl.LightningModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            # ...\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        pass\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        pass\n",
    "\n",
    "    # 학습 중간에 모델의 성능을 체크하는 용도로 사용\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        pass\n",
    "\n",
    "    # validation_step의 결과로 무엇인가 할 일이 있으면 validation_epoch_end 메서드에 작성\n",
    "    def validation_epoch_end(self, validation_step_outputs):\n",
    "        for pred in validation_step_outputs:\n",
    "            pass\n",
    "\n",
    "    # test 데이터로더에서 제공하는 배치를 가지고 확인하고 싶은 통계량을 기록하는데 사용\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        pass\n",
    "\n",
    "    # 모델의 최적 파라미터를 찾을 때 사용할 optimizer와 scheduler를 구현\n",
    "    def configure_optimizers(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 확인 결과 GPU PyTorch와 PyTorch ligthing 사이에 호환성 문제 해결 안됨!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Boston House 예제로 소스 설명"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas\n",
      "  Using cached pandas-2.2.3-cp311-cp311-win_amd64.whl.metadata (19 kB)\n",
      "Requirement already satisfied: numpy>=1.23.2 in d:\\01_programming\\100_hugobank\\mine\\study-pytorch\\torch-cpu-env\\lib\\site-packages (from pandas) (2.2.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in d:\\01_programming\\100_hugobank\\mine\\study-pytorch\\torch-cpu-env\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1 (from pandas)\n",
      "  Using cached pytz-2024.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas)\n",
      "  Downloading tzdata-2025.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: six>=1.5 in d:\\01_programming\\100_hugobank\\mine\\study-pytorch\\torch-cpu-env\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Using cached pandas-2.2.3-cp311-cp311-win_amd64.whl (11.6 MB)\n",
      "Using cached pytz-2024.2-py2.py3-none-any.whl (508 kB)\n",
      "Downloading tzdata-2025.1-py2.py3-none-any.whl (346 kB)\n",
      "   ---------------------------------------- 0.0/346.8 kB ? eta -:--:--\n",
      "   - -------------------------------------- 10.2/346.8 kB ? eta -:--:--\n",
      "   -------- ------------------------------ 71.7/346.8 kB 975.2 kB/s eta 0:00:01\n",
      "   -------------------------------------- - 337.9/346.8 kB 3.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 346.8/346.8 kB 2.7 MB/s eta 0:00:00\n",
      "Installing collected packages: pytz, tzdata, pandas\n",
      "Successfully installed pandas-2.2.3 pytz-2024.2 tzdata-2025.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# 판다스 설치\n",
    "!pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "from torch import Tensor, nn\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn import functional as F\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boston 집값 데이터를 읽어온다.\n",
    "boston = fetch_openml(name='boston', version=1)  # 대신 openml에서 가져오기\n",
    "X, y = boston.data, boston.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SklearnDataset(Dataset):\n",
    "    def __init__(self, X: np.ndarray, y: np.ndarray):\n",
    "        '''\n",
    "        데이터셋 초기화 함수\n",
    "        X: 입력 데이터 (특성)\n",
    "        y: 타겟 데이터 (레이블)\n",
    "        '''\n",
    "        super().__init__()\n",
    "        scaler = MinMaxScaler()         # MinMaxScaler를 사용하여 데이터를 0과 1 사이로 정규화\n",
    "\n",
    "        scaler.fit(X)                   # 입력 데이터를 학습하여 정규화 파라미터를 계산\n",
    "        self.X = scaler.transform(X)    # 입력 데이터 X를 정규화된 값으로 변환\n",
    "        self.Y = y                      # 타겟 데이터 y는 그대로 저장\n",
    "\n",
    "    def __len__(self):\n",
    "        '''\n",
    "        데이터셋의 크기 (샘플의 개수)를 반환하는 함수\n",
    "        '''\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        '''    \n",
    "        주어진 인덱스 idx에 해당하는 입력 데이터 X와 타겟 데이터 y를 반환하는 함수\n",
    "        X는 np.float32 형으로 변환되고, y도 np.float32 형으로 변환됨\n",
    "        '''\n",
    "        x = self.X[idx].astype(np.float32)  # X 데이터를 float32로 변환\n",
    "        y = self.Y[idx].astype(np.float32)  # y 데이터를 float32로 변환\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 위에서 정의한 SklearnDataset 클래스로 인스턴스 생성\n",
    "bostonds = SklearnDataset(X, y)\n",
    "\n",
    "# DataLoader 객체 생성\n",
    "train_loader = DataLoader(\n",
    "                    bostonds,           # 데이터셋 객체 (여기서는 SklearnDataset)\n",
    "                    batch_size=32,      # 배치 크기 설정 (한 번에 32개의 샘플을 처리)\n",
    "                    shuffle=True,       # 데이터를 랜덤하게 섞어서 학습에 사용\n",
    "                    drop_last=True,     # 마지막 배치가 미니 배치 크기보다 작을 경우 버리기\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 선형 회귀 모델을 위한 클래스 정의\n",
    "class LinRegModel(pl.LightningModule):\n",
    "    \n",
    "    # 초기화 메서드. 모델을 정의하고, 사용할 신경망 레이어를 설정\n",
    "    def __init__(self, input_dim: int):\n",
    "        '''\n",
    "        모델의 입력 차원과 출력 차원을 정의\n",
    "        input_dim: 입력 데이터의 특성 수 (여기서는 13개 특성)\n",
    "        '''\n",
    "        super().__init__()\n",
    "        # 선형 회귀 레이어 정의: 입력 차원(input_dim)과 출력 차원(1)을 설정\n",
    "        self.linear = nn.Linear(in_features=13, out_features=1, bias=True)\n",
    "\n",
    "    # 순전파 함수. 입력 x에 대해 모델의 예측값을 계산\n",
    "    # 모델의 추론 결과를 제공하고 싶을 때 사용\n",
    "    def forward(self, x):\n",
    "        '''모델의 입력 x를 받아 예측값 y_hat을 계산'''\n",
    "        y_hat = self.linear(x)      # 선형 회귀 레이어를 통해 예측값 계산\n",
    "        return y_hat\n",
    "\n",
    "    # 훈련 과정에서 한 배치에 대해 손실을 계산\n",
    "    # 단일 배치에서의 손실을 반환. train loop로 자동 반복\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        '''훈련 데이터의 한 배치(batch)에서 입력(x)와 출력(y)을 받아 손실(loss)을 계산'''\n",
    "        x, y = batch        # 배치에서 입력(x)와 출력(y)을 추출\n",
    "        x = x.view(x.size(0), -1)       # 입력을 평탄화 (2D 텐서로 변환)\n",
    "        y_hat = self(x)         # 모델을 통해 예측값 y_hat 계산\n",
    "        loss = F.mse_loss(y_hat, y, reduction=\"sum\")        # 평균 제곱 오차(MSE) 손실 계산\n",
    "        return loss\n",
    "\n",
    "    # 옵티마이저 설정 (Adam 옵티마이저 사용)\n",
    "    def configure_optimizers(self):\n",
    "        '''\n",
    "        모델 파라미터에 대한 옵티마이저를 설정합니다. \n",
    "        여기서는 Adam 옵티마이저를 사용'''\n",
    "        return torch.optim.Adam(self.parameters(), lr=1e-4)  # 학습률 1e-4로 Adam 옵티마이저 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "d:\\01_Programming\\100_HugoBank\\Mine\\study-pytorch\\torch-cpu-env\\Lib\\site-packages\\pytorch_lightning\\loops\\utilities.py:73: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "\n",
      "  | Name   | Type   | Params | Mode \n",
      "------------------------------------------\n",
      "0 | linear | Linear | 14     | train\n",
      "------------------------------------------\n",
      "14        Trainable params\n",
      "0         Non-trainable params\n",
      "14        Total params\n",
      "0.000     Total estimated model params size (MB)\n",
      "1         Modules in train mode\n",
      "0         Modules in eval mode\n",
      "d:\\01_Programming\\100_HugoBank\\Mine\\study-pytorch\\torch-cpu-env\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "d:\\01_Programming\\100_HugoBank\\Mine\\study-pytorch\\torch-cpu-env\\Lib\\site-packages\\pytorch_lightning\\loops\\fit_loop.py:310: The number of training batches (15) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2:  27%|██▋       | 4/15 [00:00<00:00, 276.02it/s, v_num=1] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\perso\\AppData\\Local\\Temp\\ipykernel_29588\\1653378779.py:15: UserWarning: Using a target size (torch.Size([32])) that is different to the input size (torch.Size([32, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  loss = F.mse_loss(y_hat, y, reduction=\"sum\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 999: 100%|██████████| 15/15 [00:00<00:00, 319.14it/s, v_num=1]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1000` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 999: 100%|██████████| 15/15 [00:00<00:00, 267.80it/s, v_num=1]\n"
     ]
    }
   ],
   "source": [
    "# PyTorch Lightning Trainer 객체 생성\n",
    "trainer = pl.Trainer()              # 모델 훈련을 위한 Trainer 객체 생성. 기본 설정으로 실행\n",
    "# 모델 인스턴스 생성\n",
    "model = LinRegModel(input_dim=13)   # LinRegModel을 생성하고 입력 차원(input_dim)을 13으로 설정\n",
    "# 모델 훈련 시작\n",
    "trainer.fit(model, train_loader)    # train_loader에서 제공되는 데이터로 모델을 훈련"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch-cpu-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
