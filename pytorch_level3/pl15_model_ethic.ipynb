{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 딥러닝 모델의 윤리 및 책임감 있는 AI 개발\n",
    "\n",
    "### 내용\n",
    "1. AI와 딥러닝 모델의 윤리적 문제\n",
    "    - AI 윤리\n",
    "        - AI와 딥러닝 모델의 사회적 영향력\n",
    "        - 윤리적 문제와 책임감 있는 AI 개발의 필요성\n",
    "\n",
    "    - 주요 윤리적 문제\n",
    "        - 편향성(Bias)\n",
    "        - 프라이버시(Privacy)\n",
    "        - 투명성(Transparency)\n",
    "        - 공정성(Fairness)\n",
    "        - 신뢰성(Reliability)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 예제 나중에"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 최신 딥러닝 연구 및 기술 동향\n",
    "\n",
    "### 강의내용\n",
    "1. 최신 딥러닝 연구 동향\n",
    "\n",
    "2. 자연어 처리(NLP) 최신 기술\n",
    "\n",
    "    - Transformer 기반 모델\n",
    "        - Transformer 개념 및 구조\n",
    "        - BERT, GPT 등의 최신 언어 모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BERT를 활용한 문서 분류 예제\n",
    "\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "# BERT 모델과 토크나이저 로드\n",
    "model_name = \"bert-base-uncased\"\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = BertForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "# 데이터셋 준비\n",
    "texts = [\"I love this movie!\", \"This was a terrible film.\"]\n",
    "labels = [1, 0]\n",
    "inputs = tokenizer(texts, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "# 훈련 인자 설정\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10,\n",
    ")\n",
    "\n",
    "# Trainer 생성\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=inputs,\n",
    "    eval_dataset=inputs\n",
    ")\n",
    "\n",
    "# 모델 훈련\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. 생성 모델(Generative Models) 최신 기술\n",
    "\n",
    "    - GAN 및 변이형 오토인코더(VAE)\n",
    "        - GAN 개념 및 구조\n",
    "        - VAE 개념 및 구조\n",
    "\n",
    "    - VAE를 활용한 이미지 생성 예제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# VAE 모델 정의\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VAE, self).__init__()\n",
    "        self.fc1 = nn.Linear(784, 400)\n",
    "        self.fc21 = nn.Linear(400, 20)\n",
    "        self.fc22 = nn.Linear(400, 20)\n",
    "        self.fc3 = nn.Linear(20, 400)\n",
    "        self.fc4 = nn.Linear(400, 784)\n",
    "\n",
    "    def encode(self, x):\n",
    "        h1 = torch.relu(self.fc1(x))\n",
    "        return self.fc21(h1), self.fc22(h1)\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def decode(self, z):\n",
    "        h3 = torch.relu(self.fc3(z))\n",
    "        return torch.sigmoid(self.fc4(h3))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x.view(-1, 784))\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return self.decode(z), mu, logvar\n",
    "\n",
    "# 손실 함수 정의\n",
    "def loss_function(recon_x, x, mu, logvar):\n",
    "    BCE = nn.functional.binary_cross_entropy(recon_x, x.view(-1, 784), reduction='sum')\n",
    "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    return BCE + KLD\n",
    "\n",
    "# 데이터셋 로드 및 전처리\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "train_dataset = datasets.MNIST('./data', train=True, download=True, transform=transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "\n",
    "# 모델, 옵티마이저 설정\n",
    "model = VAE()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# 모델 학습\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for data, _ in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        recon_batch, mu, logvar = model(data)\n",
    "        loss = loss_function(recon_batch, data, mu, logvar)\n",
    "        loss.backward()\n",
    "        train_loss += loss.item()\n",
    "        optimizer.step()\n",
    "    print(f'Epoch {epoch+1}, Loss: {train_loss/len(train_loader.dataset)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. 강화 학습(Reinforcement Learning) 최신 기술\n",
    "\n",
    "    - 강화 학습 개념 및 주요 알고리즘\n",
    "        - DQN, PPO, A3C 등의 최신 강화 학습 알고리즘\n",
    "    - Proximal Policy Optimization(PPO) 알고리즘 예제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "# 환경 설정\n",
    "env = gym.make('CartPole-v1')\n",
    "\n",
    "# PPO 모델 정의\n",
    "class Policy(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Policy, self).__init__()\n",
    "        self.affine = nn.Linear(4, 128)\n",
    "        self.action_head = nn.Linear(128, 2)\n",
    "        self.value_head = nn.Linear(128, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.affine(x))\n",
    "        action_scores = self.action_head(x)\n",
    "        state_values = self.value_head(x)\n",
    "        return torch.softmax(action_scores, dim=-1), state_values\n",
    "\n",
    "# PPO 에이전트 정의\n",
    "class PPOAgent:\n",
    "    def __init__(self):\n",
    "        self.model = Policy()\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=1e-2)\n",
    "        self.eps_clip = 0.2\n",
    "        self.gamma = 0.99\n",
    "\n",
    "    def select_action(self, state):\n",
    "        state = torch.from_numpy(state).float().unsqueeze(0)\n",
    "        probs, _ = self.model(state)\n",
    "        m = Categorical(probs)\n",
    "        action = m.sample()\n",
    "        return action.item(), m.log_prob(action)\n",
    "\n",
    "    def update(self, rewards, log_probs, state_values):\n",
    "        returns = []\n",
    "        R = 0\n",
    "        for r in rewards[::-1]:\n",
    "            R = r + self.gamma * R\n",
    "            returns.insert(0, R)\n",
    "        returns = torch.tensor(returns)\n",
    "        log_probs = torch.cat(log_probs)\n",
    "        state_values = torch.cat(state_values)\n",
    "        advantages = returns - state_values\n",
    "\n",
    "        policy_loss = []\n",
    "        value_loss = []\n",
    "        for log_prob, value, advantage in zip(log_probs, state_values, advantages):\n",
    "            ratio = torch.exp(log_prob - log_prob.detach())\n",
    "            surr1 = ratio * advantage\n",
    "            surr2 = torch.clamp(ratio, 1 - self.eps_clip, 1 + self.eps_clip) * advantage\n",
    "            policy_loss.append(-torch.min(surr1, surr2))\n",
    "            value_loss.append((returns - value).pow(2))\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        loss = torch.stack(policy_loss).sum() + torch.stack(value_loss).sum()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "# PPO 학습\n",
    "agent = PPOAgent()\n",
    "num_episodes = 1000\n",
    "for i_episode in range(num_episodes):\n",
    "    state = env.reset()\n",
    "    log_probs = []\n",
    "    state_values = []\n",
    "    rewards = []\n",
    "    for t in range(1000):\n",
    "        action, log_prob = agent.select_action(state)\n",
    "        state, reward, done, _ = env.step(action)\n",
    "        rewards.append(reward)\n",
    "        log_probs.append(log_prob)\n",
    "        state_values.append(agent.model(torch.from_numpy(state).float().unsqueeze(0))[1])\n",
    "        if done:\n",
    "            break\n",
    "    agent.update(rewards, log_probs, state_values)\n",
    "    if i_episode % 100 == 0:\n",
    "        print(f'Episode {i_episode}, Total Reward: {sum(rewards)}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
