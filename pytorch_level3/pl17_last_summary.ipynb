{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 최종 정리\n",
    "\n",
    "### 핵심 개념과 기능\n",
    "\n",
    "PyTorch는 페이스북의 AI 연구팀에서 개발한 오픈 소스 딥러닝 프레임워크로, 특히 연구와 개발 환경에서 많이 사용됩니다. PyTorch는 텐서 계산 및 동적 계산 그래프를 기반으로 하며, 이를 통해 복잡한 모델을 쉽게 설계하고 디버깅할 수 있습니다. PyTorch의 핵심 개념과 기능을 소개하겠습니다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 텐서(Tensor)\n",
    "    - 텐서는 PyTorch의 기본 데이터 구조입니다. 텐서는 N차원의 배열로, 수학적 연산을 수행하는 데 사용됩니다.\n",
    "\n",
    "    - 텐서는 torch.Tensor 클래스를 사용하여 생성할 수 있습니다. 예를 들어, 2x3 행렬을 생성하려면 다음과 같습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.Tensor([[1, 2, 3], [4, 5, 6]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. 자동 미분(Autograd)\n",
    "    - Autograd는 PyTorch의 자동 미분 엔진입니다. 계산 그래프를 동적으로 생성하고, 역전파(backpropagation)를 통해 그래디언트를 자동으로 계산합니다.\n",
    "\n",
    "    - requires_grad=True로 설정된 텐서는 모든 연산을 추적하며, backward() 메서드를 호출하면 그래디언트를 계산합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 6.,  8., 10.])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([1.0, 2.0, 3.0], requires_grad=True)\n",
    "y = x + 2\n",
    "z = y * y * 3\n",
    "z = z.mean()\n",
    "z.backward()\n",
    "print(x.grad)  # x에 대한 그래디언트 출력"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. 신경망(Neural Network)\n",
    "    - PyTorch는 torch.nn 모듈을 통해 신경망을 쉽게 구성할 수 있는 도구를 제공합니다. nn.Module 클래스를 상속하여 새로운 신경망 모듈을 정의할 수 있습니다.\n",
    "\n",
    "    - 간단한 선형 회귀 모델을 정의하려면 다음과 같습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class LinearRegressionModel(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(LinearRegressionModel, self).__init__()\n",
    "        self.linear = nn.Linear(input_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.linear(x)\n",
    "        return out\n",
    "\n",
    "model = LinearRegressionModel(1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. 최적화(Optimization)\n",
    "    - PyTorch는 torch.optim 모듈을 통해 다양한 최적화 알고리즘을 제공합니다. 일반적으로 사용되는 알고리즘으로는 SGD, Adam 등이 있습니다.\n",
    "\n",
    "    - SGD를 사용하여 모델을 최적화하려면 다음과 같습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Loss: 73.0507\n",
      "Epoch [2/100], Loss: 50.6934\n",
      "Epoch [3/100], Loss: 35.1801\n",
      "Epoch [4/100], Loss: 24.4158\n",
      "Epoch [5/100], Loss: 16.9465\n",
      "Epoch [6/100], Loss: 11.7638\n",
      "Epoch [7/100], Loss: 8.1676\n",
      "Epoch [8/100], Loss: 5.6722\n",
      "Epoch [9/100], Loss: 3.9407\n",
      "Epoch [10/100], Loss: 2.7392\n",
      "Epoch [11/100], Loss: 1.9054\n",
      "Epoch [12/100], Loss: 1.3269\n",
      "Epoch [13/100], Loss: 0.9255\n",
      "Epoch [14/100], Loss: 0.6469\n",
      "Epoch [15/100], Loss: 0.4535\n",
      "Epoch [16/100], Loss: 0.3193\n",
      "Epoch [17/100], Loss: 0.2262\n",
      "Epoch [18/100], Loss: 0.1616\n",
      "Epoch [19/100], Loss: 0.1167\n",
      "Epoch [20/100], Loss: 0.0855\n",
      "Epoch [21/100], Loss: 0.0638\n",
      "Epoch [22/100], Loss: 0.0488\n",
      "Epoch [23/100], Loss: 0.0383\n",
      "Epoch [24/100], Loss: 0.0310\n",
      "Epoch [25/100], Loss: 0.0259\n",
      "Epoch [26/100], Loss: 0.0224\n",
      "Epoch [27/100], Loss: 0.0199\n",
      "Epoch [28/100], Loss: 0.0181\n",
      "Epoch [29/100], Loss: 0.0169\n",
      "Epoch [30/100], Loss: 0.0160\n",
      "Epoch [31/100], Loss: 0.0154\n",
      "Epoch [32/100], Loss: 0.0149\n",
      "Epoch [33/100], Loss: 0.0145\n",
      "Epoch [34/100], Loss: 0.0143\n",
      "Epoch [35/100], Loss: 0.0140\n",
      "Epoch [36/100], Loss: 0.0139\n",
      "Epoch [37/100], Loss: 0.0137\n",
      "Epoch [38/100], Loss: 0.0136\n",
      "Epoch [39/100], Loss: 0.0135\n",
      "Epoch [40/100], Loss: 0.0134\n",
      "Epoch [41/100], Loss: 0.0133\n",
      "Epoch [42/100], Loss: 0.0132\n",
      "Epoch [43/100], Loss: 0.0131\n",
      "Epoch [44/100], Loss: 0.0130\n",
      "Epoch [45/100], Loss: 0.0130\n",
      "Epoch [46/100], Loss: 0.0129\n",
      "Epoch [47/100], Loss: 0.0128\n",
      "Epoch [48/100], Loss: 0.0127\n",
      "Epoch [49/100], Loss: 0.0126\n",
      "Epoch [50/100], Loss: 0.0126\n",
      "Epoch [51/100], Loss: 0.0125\n",
      "Epoch [52/100], Loss: 0.0124\n",
      "Epoch [53/100], Loss: 0.0123\n",
      "Epoch [54/100], Loss: 0.0123\n",
      "Epoch [55/100], Loss: 0.0122\n",
      "Epoch [56/100], Loss: 0.0121\n",
      "Epoch [57/100], Loss: 0.0121\n",
      "Epoch [58/100], Loss: 0.0120\n",
      "Epoch [59/100], Loss: 0.0119\n",
      "Epoch [60/100], Loss: 0.0118\n",
      "Epoch [61/100], Loss: 0.0118\n",
      "Epoch [62/100], Loss: 0.0117\n",
      "Epoch [63/100], Loss: 0.0116\n",
      "Epoch [64/100], Loss: 0.0116\n",
      "Epoch [65/100], Loss: 0.0115\n",
      "Epoch [66/100], Loss: 0.0114\n",
      "Epoch [67/100], Loss: 0.0114\n",
      "Epoch [68/100], Loss: 0.0113\n",
      "Epoch [69/100], Loss: 0.0112\n",
      "Epoch [70/100], Loss: 0.0112\n",
      "Epoch [71/100], Loss: 0.0111\n",
      "Epoch [72/100], Loss: 0.0110\n",
      "Epoch [73/100], Loss: 0.0110\n",
      "Epoch [74/100], Loss: 0.0109\n",
      "Epoch [75/100], Loss: 0.0108\n",
      "Epoch [76/100], Loss: 0.0108\n",
      "Epoch [77/100], Loss: 0.0107\n",
      "Epoch [78/100], Loss: 0.0106\n",
      "Epoch [79/100], Loss: 0.0106\n",
      "Epoch [80/100], Loss: 0.0105\n",
      "Epoch [81/100], Loss: 0.0104\n",
      "Epoch [82/100], Loss: 0.0104\n",
      "Epoch [83/100], Loss: 0.0103\n",
      "Epoch [84/100], Loss: 0.0103\n",
      "Epoch [85/100], Loss: 0.0102\n",
      "Epoch [86/100], Loss: 0.0101\n",
      "Epoch [87/100], Loss: 0.0101\n",
      "Epoch [88/100], Loss: 0.0100\n",
      "Epoch [89/100], Loss: 0.0099\n",
      "Epoch [90/100], Loss: 0.0099\n",
      "Epoch [91/100], Loss: 0.0098\n",
      "Epoch [92/100], Loss: 0.0098\n",
      "Epoch [93/100], Loss: 0.0097\n",
      "Epoch [94/100], Loss: 0.0097\n",
      "Epoch [95/100], Loss: 0.0096\n",
      "Epoch [96/100], Loss: 0.0095\n",
      "Epoch [97/100], Loss: 0.0095\n",
      "Epoch [98/100], Loss: 0.0094\n",
      "Epoch [99/100], Loss: 0.0094\n",
      "Epoch [100/100], Loss: 0.0093\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "# 훈련 루프\n",
    "for epoch in range(100):\n",
    "    inputs = torch.tensor([[1.0], [2.0], [3.0], [4.0]])\n",
    "    targets = torch.tensor([[2.0], [4.0], [6.0], [8.0]])\n",
    "\n",
    "    # 순전파\n",
    "    outputs = model(inputs)\n",
    "    loss = criterion(outputs, targets)\n",
    "\n",
    "    # 역전파 및 최적화\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/100], Loss: {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. 데이터 처리(Data Loading)\n",
    "    - PyTorch는 torch.utils.data 모듈을 통해 데이터셋을 쉽게 처리할 수 있는 도구를 제공합니다. Dataset과 DataLoader 클래스를 사용하여 데이터셋을 정의하고, 배치 단위로 데이터를 로드할 수 있습니다.\n",
    "\n",
    "    - 간단한 데이터셋을 정의하고 로드하려면 다음과 같습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[4.],\n",
      "        [3.]]) tensor([[8.],\n",
      "        [6.]])\n",
      "tensor([[2.],\n",
      "        [1.]]) tensor([[4.],\n",
      "        [2.]])\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class SimpleDataset(Dataset):\n",
    "    def __init__(self):\n",
    "        self.data = torch.tensor([[1.0], [2.0], [3.0], [4.0]])\n",
    "        self.labels = torch.tensor([[2.0], [4.0], [6.0], [8.0]])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.labels[idx]\n",
    "\n",
    "dataset = SimpleDataset()\n",
    "dataloader = DataLoader(dataset, batch_size=2, shuffle=True)\n",
    "\n",
    "for data, labels in dataloader:\n",
    "    print(data, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. GPU 지원\n",
    "    - PyTorch는 GPU를 사용하여 연산을 가속화할 수 있습니다. GPU를 사용하려면 텐서나 모델을 GPU로 옮기면 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegressionModel(\n",
       "  (linear): Linear(in_features=1, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. 커스텀 레이어 및 모델 정의\n",
    "    - PyTorch는 nn.Module을 상속받아 사용자 정의 레이어와 모델을 쉽게 구현할 수 있습니다. 이를 통해 복잡한 모델 아키텍처를 만들 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "class CustomLayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CustomLayer, self).__init__()\n",
    "        self.linear = nn.Linear(10, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear(x)\n",
    "        return torch.relu(x)\n",
    "\n",
    "class CustomModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CustomModel, self).__init__()\n",
    "        self.layer1 = CustomLayer()\n",
    "        self.layer2 = nn.Linear(10, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        return x\n",
    "\n",
    "model = CustomModel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. 학습률 스케줄러(Learning Rate Schedulers)\n",
    "    - PyTorch의 torch.optim.lr_scheduler 모듈은 학습률을 동적으로 조절할 수 있는 다양한 스케줄러를 제공합니다. 예를 들어, StepLR을 사용하여 일정 간격마다 학습률을 감소시킬 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\perso\\.pyenv\\pyenv-win\\versions\\3.11.9\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:224: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "scheduler = lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.1)\n",
    "\n",
    "for epoch in range(100):\n",
    "    # 훈련 코드\n",
    "    # ...\n",
    "\n",
    "    # 학습률 업데이트\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. 데이터 증강(Data Augmentation)\n",
    "    - 데이터 증강은 모델의 일반화 성능을 높이기 위해 입력 데이터를 다양한 방식으로 변형하는 기법입니다. PyTorch는 `torchvision.transforms` 모듈을 통해 이미지 데이터에 대한 다양한 데이터 증강 기능을 제공합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# Dataset과 함께 사용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. 혼합 정밀도 훈련(Mixed Precision Training)\n",
    "    - 혼합 정밀도 훈련은 모델의 일부 연산을 16비트 부동 소수점(float16)으로 수행하여 메모리 사용량을 줄이고 계산 속도를 높이는 기술입니다. PyTorch는 `torch.cuda.amp` 모듈을 통해 이를 지원합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.cuda.amp import GradScaler, autocast\n",
    "\n",
    "scaler = GradScaler()\n",
    "\n",
    "for data, target in dataloader:\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    with autocast():\n",
    "        output = model(data)\n",
    "        loss = loss_fn(output, target)\n",
    "\n",
    "    scaler.scale(loss).backward()\n",
    "    scaler.step(optimizer)\n",
    "    scaler.update()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10. 사용자 정의 데이터셋(Custom Dataset) 및 데이터 로더(Data Loader)\n",
    "    - PyTorch의 `torch.utils.data.Dataset`을 상속받아 사용자 정의 데이터셋을 만들 수 있으며, DataLoader를 사용하여 효율적으로 데이터를 로드할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data, labels):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.labels[idx]\n",
    "\n",
    "data = torch.randn(100, 10)\n",
    "labels = torch.randn(100, 1)\n",
    "dataset = CustomDataset(data, labels)\n",
    "dataloader = DataLoader(dataset, batch_size=4, shuffle=True)\n",
    "\n",
    "for batch in dataloader:\n",
    "    inputs, targets = batch\n",
    "    # 훈련 코드"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "11. 모델 체크포인트 저장 및 로드(Model Checkpointing)\n",
    "    - 훈련 중간에 모델을 저장하고 나중에 불러올 수 있습니다. 이를 통해 훈련이 중단되더라도 이어서 계속할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 저장\n",
    "torch.save(model.state_dict(), 'model.pth')\n",
    "\n",
    "# 모델 로드\n",
    "model = CustomModel()\n",
    "model.load_state_dict(torch.load('model.pth'))\n",
    "model.eval()  # 추론 모드로 전환"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "12. 텐서보드 지원(TensorBoard Support)\n",
    "    - PyTorch는 텐서보드를 통해 훈련 과정을 시각화할 수 있도록 지원합니다. 이를 통해 손실, 정확도, 그래디언트 등의 메트릭을 모니터링할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "writer = SummaryWriter()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for batch_idx, (data, target) in enumerate(dataloader):\n",
    "        # 훈련 코드\n",
    "        writer.add_scalar('Loss/train', loss.item(), epoch * len(dataloader) + batch_idx)\n",
    "\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "13. 모델 병렬 처리(Model Parallelism)\n",
    "    - 대규모 모델을 여러 GPU에 분산하여 훈련할 수 있습니다. PyTorch는 `torch.nn.DataParallel`과 `torch.nn.parallel.DistributedDataParallel`을 통해 이를 지원합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.DataParallel(CustomModel())\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "14. 전이 학습(Transfer Learning)\n",
    "    - 전이 학습은 사전 학습된 모델을 사용하여 새로운 작업을 빠르게 해결할 수 있는 기법입니다. 일반적으로 대규모 데이터셋에서 학습된 모델의 가중치를 초기화하거나, 일부 레이어를 고정(freeze)하고 새로운 데이터에 맞게 나머지 레이어를 학습시킵니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.models as models\n",
    "\n",
    "# 사전 학습된 ResNet 모델 로드\n",
    "model = models.resnet18(pretrained=True)\n",
    "\n",
    "# 마지막 레이어를 새로운 작업에 맞게 수정\n",
    "num_features = model.fc.in_features\n",
    "model.fc = nn.Linear(num_features, num_classes)\n",
    "\n",
    "# 특정 레이어 고정\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# 새로운 레이어만 학습\n",
    "for param in model.fc.parameters():\n",
    "    param.requires_grad = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "15. 하이퍼파라미터 튜닝(Hyperparameter Tuning)\n",
    "    - 최적의 하이퍼파라미터를 찾기 위해 다양한 검색 방법을 사용할 수 있습니다. `optuna`, `ray.tune` 등의 라이브러리를 사용하여 하이퍼파라미터 검색을 자동화할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "\n",
    "def objective(trial):\n",
    "    lr = trial.suggest_loguniform('lr', 1e-5, 1e-1)\n",
    "    batch_size = trial.suggest_int('batch_size', 16, 128)\n",
    "    model = create_model()  # 사용자 정의 모델 생성 함수\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    # 훈련 및 검증 코드\n",
    "    return validation_loss\n",
    "\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=100)\n",
    "print(study.best_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "16. 딥러닝 라이브러리 및 프레임워크 통합\n",
    "    - PyTorch는 다른 딥러닝 프레임워크 및 라이브러리와 통합하여 사용할 수 있습니다. 예를 들어, **Hugging Face Transformers**를 사용하여 자연어 처리 모델을 쉽게 다룰 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "inputs = tokenizer(\"Hello, world!\", return_tensors=\"pt\")\n",
    "outputs = model(**inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "17. 모델 해석 및 설명(Explainable AI)\n",
    "    - 모델의 예측 결과를 해석하고 설명하는 것은 중요합니다. `SHAP`, `LIME` 같은 라이브러리를 사용하여 모델의 예측을 설명할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "\n",
    "# 데이터 준비\n",
    "data = ...\n",
    "\n",
    "# 모델 준비\n",
    "model = ...\n",
    "\n",
    "# SHAP 값을 계산하고 시각화\n",
    "explainer = shap.Explainer(model, data)\n",
    "shap_values = explainer(data)\n",
    "shap.summary_plot(shap_values, data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "18. 강화 학습(Reinforcement Learning)\n",
    "    - PyTorch를 사용하여 강화 학습 알고리즘을 구현할 수 있습니다. `OpenAI Gym`과 같은 환경과 함께 사용하여 강화 학습 모델을 학습시킬 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# 환경 설정\n",
    "env = gym.make('CartPole-v1')\n",
    "\n",
    "# 모델 정의\n",
    "class PolicyNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PolicyNetwork, self).__init__()\n",
    "        self.fc = nn.Linear(4, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.softmax(self.fc(x), dim=-1)\n",
    "\n",
    "model = PolicyNetwork()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# 강화 학습 코드\n",
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "19. 컴퓨터 비전 및 자연어 처리(CV/NLP)\n",
    "    - PyTorch는 컴퓨터 비전과 자연어 처리 분야에서도 강력한 도구입니다. `torchvision`과 `torchtext` 라이브러리를 사용하여 다양한 데이터셋과 모델을 쉽게 활용할 수 있습니다.\n",
    "\n",
    "#### 컴퓨터 비전"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import CIFAR10\n",
    "\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
    "train_dataset = CIFAR10(root='./data', train=True, transform=transform, download=True)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 자연어 처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data import Field, TabularDataset, BucketIterator\n",
    "\n",
    "TEXT = Field(tokenize='spacy', lower=True)\n",
    "LABEL = Field(sequential=False, use_vocab=False)\n",
    "\n",
    "datafields = [('text', TEXT), ('label', LABEL)]\n",
    "train, val, test = TabularDataset.splits(path='data', train='train.csv', validation='val.csv', test='test.csv', format='csv', fields=datafields)\n",
    "\n",
    "TEXT.build_vocab(train, vectors=\"glove.6B.100d\")\n",
    "train_iter, val_iter, test_iter = BucketIterator.splits((train, val, test), batch_size=32, sort_within_batch=True, sort_key=lambda x: len(x.text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "20. 배포(Deployment)\n",
    "    - 학습된 모델을 실제 서비스에 배포하는 것도 중요합니다. PyTorch는 `torch.jit`을 사용하여 모델을 TorchScript로 변환하고, 이를 통해 모델을 최적화하여 배포할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# 모델 정의 및 학습\n",
    "model = ...\n",
    "\n",
    "# TorchScript로 변환\n",
    "scripted_model = torch.jit.script(model)\n",
    "\n",
    "# 모델 저장\n",
    "torch.jit.save(scripted_model, 'model.pt')\n",
    "\n",
    "# 모델 로드 및 추론\n",
    "loaded_model = torch.jit.load('model.pt')\n",
    "output = loaded_model(input_tensor)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
